怎么说呢……这两天看深度学习的相关内容，里面关于感知机的介绍，以及感知机+非线性激活函数构成复杂的DNN（深度神经网络），总觉得哪里怪怪的。于是就找了找还有哪些结构的NN。看了几个诸如RNN，CNN之类的，其中有一个描述一直很在意：

它们几乎都在用**层**（当然，还有宽度等指标）这个指标来描述神经网络的规模。而层这个概念，我个人感觉这多少约束了ANN的结构灵活性。同时，这东西好像也逃不开自动机的本质？

而且在学习ANN的时候，我似乎下意识把NN当成了实现AI的必经之路。事实上这二者现在还不能说完全相关乃至等价，因为ANN能否产生智能还是未知的。部分人的工作现在应该算是搞ANN部分产物的应用，而这和研究ANN与智能的关系不太大，毕竟ANN的目的是AI而非做一个分类器（虽然好像这玩意确实挺实用）。还有一个叫进化算法的东西？听名字瞎猜感觉应该挺有意思，想起来了以前的自修改程序，不过那都属于旧时代编程黑魔法的范畴了。

## 为啥是非线性激活函数？

官方答案是：因为逻辑代数。就用上面的DNN说明，如果每一层的激活函数都是线性函数，那么不同层的组合必定能化简为一层（这里注意，DNN是无环图）。所以他们就用非线性函数当激活函数咯，比如现在用的很多的$sigmoid()$函数就是。类比于函数的泰勒展开，已经有证明，这样的DNN可以模拟**任意一个非线性函数**。

我个人理解嘛，就是非线性激活函数也许也能用代数化简那么一点点，但是非线性使得化简后的复杂性降低程度并不大。以及还有个问题，就是上一段的最后一句，这DNN到头来就是函数拟合工具？这……感觉虽然也挺有用吧（能作为部分场合求解复杂函数的工具），但是这样感觉DNN就更靠近数学方法，更类似于一种解题技巧，而非更有深度的东西。所以多少感觉上限低了有点遗憾（假设我没理解错的话）？

## 关于网络结构方面

看到一个很有意思的说法，决定网络是否具有智能的，可能是网络结构，也有可能是网络状态。这就类似于状态机了。不过状态机好像状态数量是有穷的？

看到了一个叫液态机的东西：

>液态机是一种特殊的脉冲神经网络。液态机由大量的神经元组成。这里，每个节点接收来自外部源和其它节点的输入，这些输入可能随时间而变化。请注意，液态机上的节点是随机连接的。在液态机中，激活函数替换为阈值级别。只有当液态机达到阈值水平时，一个特定的神经元才会发出输出。

还有另一个东西，霍菲特网络。

>在 Hopfield 神经网络中，每个神经元都与其它神经元直接相连。在这个网络中，神经元要么是开的，要么是关的。神经元的状态可以通过接受其它神经元的输入而改变。

上面两种方式合起来，概念的严格程度稍微降低一些（比如不用所有神经元互相连接），就比较接近我所构想的神经网络结构了。

## 认知与脑科学与ANN

神经系统的信号传递是脉冲形式的，这带来两个问题。

其一，这似乎让信息传递变得相对更低效。这无可厚非，如果一直用其他形式传递，虽然效率高，但是脑负载和能耗应该不是目前人体能承受的。

其二，速度下降带来的信号异步传递问题。不过这或许不能说是问题，这似乎是一种新机制。人脑借助此形成更复杂的系统结构也有可能。

## 状态机和ANN

状态机是由转移指令和状态组成的东西，它会根据状态和指令自动转移。