# 神经网络的层级结构

神经网络是一种模仿生物神经系统的计算模型，是一种**算法**

用于解决分类、回归、生成等机器学习问题

  

层级结构通常包括输入层

（第一层）（接受参数）

隐藏层（中间很多层）

输出层（最后一层）

每一层由若干个神经元（或节点）组成

每个节点又称为感知机模型

  

深度学习就是隐藏层很多的神经网络（突出一个深度）

  

# 感知机模型

一个感知机模型可以接受很多数据（参数）作为输入

  

参数$x_i$﻿，权重$w_i$﻿，偏置$b$﻿, 输出$y$﻿

$\sum_{i = 1}^{n}x_iw_i + b => y$

# 神经网络的Pytorch API

```Python
CLASS torch.nn.linear(in_features,out_features,bias=True,device=None,dtype=None)[SOURCE]
```

这是一层神经网络的设置

in_features是输入神经元的数数量(int)

out_features是当前层神经网络的数量(int)

bias=True是启用偏置(bool)

device是指定计算时使用什么设备

dtype是指定计算的数据类型（推荐浮点型）

本质上是做一个线性变换

$y = x A^T + b$

# 神经网络的有监督训练

权重$w$﻿和偏置$b$﻿初始化后是一个随机数，没有什么特殊含义，对数据还没有什么分析能力

  

我们要把它训练好

使用有监督训练

  

训练模型时不仅仅有**对应数据**，还要有**对应的真值**

  

真值的设计和不同的任务相关

目标检测、图片识别（名字）和语义分割（边界）不同

  

模型会根据输入给出预测结果Prediction

  

这时由于模型的权重和偏置都是随机数，这是肯定**不准确**

  

现在我们需要估计这个Prediction和数据真值的Distance（差距，距离）

  

训练目的： 尽可能减小Prediction和真值的Distance

（要让Distance足够小，就可以说Prediction无限接近于真值）

  

这个Distance可以用一种叫loss function（损失函数）的方法进行表示

  

例如Prediction是$y$﻿而真值是$y'$﻿, 那么$\frac{1}{2}(y - y')^2$﻿(平方差损失)就可以衡量连这个两者间的距离

我们把这种公式都叫做loss function

  

训练模型的目的从求Distance最小这个事情等价转换成了求loss function极小值这个事情,把问题变成了一个数学模型

  

接下来要解决的是,我们要怎么求loss function的极小值,怎么求一个函数的极小值?

  

(高中:求导)(是比较低级的算法)

  

我们这里使用更高级的算法,**梯度下降法(GD)**

下面我们将知道怎么使用梯度下降法来求损失函数的极小值来帮助我们进行神经网络训练

  

# 神经网络的梯度下降算法(GD)

我们要了解梯度下降法,我们就得先知道什么是梯度

  

梯度和我们高数中的**方向**、**导数**相关,这里我们讲得不那么数学化,可以把它理解为一个**偏导的集合**

  

比如说,一个函数里面有很多自变量,我们可以对每一个自变量都求一个导数,那么这些导数的集合就称作**梯度(即偏导数）**

  

**梯度的方向是向结果(y)增大的方向在x轴上的映射(←或者→)**

  

极端的情况,如果这个函数只有一个自变量,那么梯度就退化成导数这个概念,可以看出,梯度是导数概念的推广(梯度和导数在某种程度上本质上是一样的)

  

要明白梯度在干嘛,就去理解导数在干嘛

  

对于导数,我们知道,沿着导数方向爬,是该函数增长最快的一个方向,但是现在我们不关心函数什么时候增长快,而是关心它的极小值在什么时候取到

  

自然而然地,我们想到沿着**梯度的反方向**,可以一点一点找到极小值

这就是梯度下降法的核心原理

  

公式:

$\theta = \theta' - lr \cdot g$

lr=learning rate是一个超参数(学习速率),控制着**参数更新的快慢**,如果它比较大,那么后面整体也就比较大,减去的就比较多,跑的也比较快,**很可能跑过了**,**导致错过极小值**

$\theta$﻿是新的参数向量

$\theta'$﻿是旧的参数向量

$g$﻿=gradient是损失函数的梯度(有方向,前面-号意思是沿着梯度反方向前进)

这是个矩阵运算

  

这个$\theta$﻿其实就是模型Model里的权重$w$﻿和偏置$b$﻿,排成了**矩阵**

  

把公式拆开看,例如

$w = w' - lr\cdot g$

$b = b' - lr \cdot g$

这就是参数的更新方法

我制作的matlab动画

```MATLAB
% 创建图形窗口
figure;

% 定义二次函数 y = ax^2 + bx + c
a = 2.0;  % 二次项系数
b = -3.0; % 一次项系数
c = 1.0;  % 常数项

% 绘制二次曲线
x = -5:0.1:5;
y = a * x.^2 + b * x + c;
plot(x, y);
hold on;

% 初始值
x0 = 0.0;

% 梯度下降算法
learningRate = 0.1;   % 学习率
numIterations = 100;  % 迭代次数

% 记录梯度下降过程
descentPoints = zeros(numIterations+1, 2);
descentPoints(1, :) = [x0, quadraticFunction(x0)];

% 绘制起始点
scatter(descentPoints(1, 1), descentPoints(1, 2), 'filled');

% 设置图像标题和坐标轴标签
title('Gradient Descent');
xlabel('x');
ylabel('y');

% 创建按钮
button = uicontrol('Style', 'pushbutton', 'String', 'Step', ...
    'Position', [20 20 60 20], 'Callback', @buttonCallback);

% 将初始值和学习率存储在图形对象的 UserData 中
set(gcf, 'UserData', [x0, learningRate]);

% 定义按钮点击事件处理函数
function buttonCallback(~,~)
    % 获取初始值和学习率
    data = get(gcf, 'UserData');
    x = data(1);
    learningRate = data(2);
    
    persistent i
    if isempty(i)
        i = 1;  % 初始化迭代次数
    end
    
    % 获取二次函数的系数
    a = 2.0;
    b = -3.0;
    c = 1.0;
    
    % 计算函数值和梯度
    y = quadraticFunction(x);
    gradient = 2 * a * x + b;
    
    % 更新参数
    x = x - learningRate * gradient;
    
    % 记录梯度下降过程
    descentPoints(i+1, :) = [x, quadraticFunction(x)];
    
    % 清除上一次迭代的点
    % cla;
    
    % 绘制二次曲线
    plot(x, y);
    hold on;
    
    % 绘制梯度下降过程
    scatter(descentPoints(1:i+1, 1), descentPoints(1:i+1, 2), 'filled');
    
    % 更新迭代次数
    i = i + 1;
    
    % 更新初始值和学习率
    set(gcf, 'UserData', [x, learningRate]);
end

% 定义二次函数
function y = quadraticFunction(x)
    a = 2.0;
    b = -3.0;
    c = 1.0;
    y = a * x.^2 + b * x + c;
end
```

**通过不断更新参数，使得模型输出更接近真实结果**

  

这个数据的真值相当于一种监督信号

  

# Pytorch在训练过程的作用

数据集加载

使用Dataloader类实现

  

用nn.model进行搭建神经网络

  

loss = nn.CrossEntropLoss()交叉熵损失

loss = nn.MSE()平方差损失

  

损失函数求导

loss.backward()

  

梯度下降法更新参数

optimizer.step()

  

pytorch啥都有,很简单,方便,快捷

  

# 项目实战