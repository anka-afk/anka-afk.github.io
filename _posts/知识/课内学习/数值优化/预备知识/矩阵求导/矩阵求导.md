# 标量函数

> 标量函数是一种**映射**，它把定义域这个**集合**中的元素映射为一个**实数**

这意味着标量函数不仅可以是我们所熟知的把一个数映射到另外一个数，它还可以把一个**向量**、一个**矩阵**映射到一个数。

由于在数值优化中我们会遇见大量的矩阵输入，为了了解不同的输入会对函数的求导操作造成什么样的影响，我们需要先看看不同的输入下标量函数是什么样的。

---

# 不同的函数输入

现在让我们来看看不同的输入下，函数是怎么样的

## 输入一个标量

假设我们有一个**标量**$x\in \mathbb{R}$﻿，那么我们可以得到一个例子

$f(x) = 2x+3$

这就是一个函数，它把$x$﻿这个**数**映射到$2x+3$﻿这个**数**

## 输入一个向量

假设我们有一个**向量**，$x = (x_1,x_2,x_3,x_4)\quad x_i \in \mathbb{R},i\in1,2,3,4$﻿

我们同样地可以得到一个例子

$f(x) = a_1x_1+a_2x_2+a_3x_3+a_4x_4= \sum_{i=1}^{4} a_ix_i$

这就是一个向量函数，假设我们把系数用向量表示，这个函数就能表示为矩阵的乘积

我们令$a = (a_1,a_2,a_3,a_4)$﻿,那么有

$f(x) = a\times x^T$

这里的$\times$﻿是矩阵乘法，而上标$T$﻿则是取转置的意思，此处不过多赘述

## 输入一个矩阵

假设我们有一个矩阵$x = \begin{pmatrix}x_1&x_2\\x_3&x_4\end{pmatrix}$﻿,$x$﻿的取值同上面向量中的取值

我们能够得到相同的例子

$f(x) = a_1x_1+a_2x_2+a_3x_3+a_4x_4= \sum_{i=1}^{4} a_ix_i$

$f(x) = a\times x^T$

这里我们可以看出，**向量**和**矩阵**的函数都是单变量函数的推广而已

换言之，向量和矩阵的函数其实是多变量函数的一种表现形式

---

现在我们知道了不同的输入形式带来的不同函数形态，也就是说，在上面我们都是通过改变函数的输入（也就是定义域）来观察函数的变化的

于是我们自然地想到，改变函数的值域会出现什么变化呢？如果函数不再只输出一个数，而是输出一个向量、一个矩阵，又会有什么样的结果呢？

---

# 不同的函数输出

## 输出一个标量

就像上面我们展示的那样，输出一个标量的函数，在分别输入标量、向量、矩阵时，有这样的表现形式

$f(x) = 2x+3$

$f(x) = a_1x_1+a_2x_2+a_3x_3+a_4x_4= \sum_{i=1}^{4} a_ix_i$

$f(x) = a_1x_1+a_2x_2+a_3x_3+a_4x_4= \sum_{i=1}^{4} a_ix_i$

## 输出一个向量

我们称这种函数为**实向量函数**

在分别输入标量、向量、矩阵时，有这样的表现形式

$f_{3\times 1}(x) = \begin{pmatrix}x+1\\2x+3\\3x+8\end{pmatrix}$

$f_{3\times 1}(x) = \begin{pmatrix}x_1+x_2+x_3\\x_1+2x_2+3x_3\\x_1+4x_2+5x_3\end{pmatrix}$

$f_{3\times 1}(x) = \begin{pmatrix}x_{11}+x_{12}+x_{21}+x_{22}\\x_{11}+2x_{12}+x_{21}+4x_{22}\\3x_{11}+x_{12}+7x_{21}+x_{22}\end{pmatrix}$

可以看出，向量函数其实就是标量函数的自然推广，就是把几个标量函数放在了同一个向量中而已

## 输出一个矩阵

我们称这种函数为**实矩阵函数**

在分别输入标量、向量、矩阵时，有这样的表现形式

输入一个标量$x$﻿:

$F_{3\times 2}(x) = \begin{pmatrix}x+1 &2x+1\\x^2+1&2x^2+2\\x^2+1&3x^3+3\end{pmatrix}$

可以看出，其实就是在矩阵的每一个格子中放入当输出标量时的函数形式

  

输入一个向量$x=(x_1,x_2)^T$﻿:

$F_{3\times 2}(x) = \begin{pmatrix}2x_1+x_2&x_1+7x_2\\6x_1+x_2&2x_1+3x_2\\3x_1+x_2&3x_1+8x_2\end{pmatrix}$

同理，也是往矩阵中放输出标量的形式

  

输入一个矩阵$x = \begin{pmatrix}x_1&x_2\\x_3&x_4\end{pmatrix}$﻿:

$F_{3\times 2}(x) = \begin{pmatrix}2x_1+x_2+3x_3+2x_4&x_1+7x_2+2x_3+9x+4\\6x_1+x_2+2x_3+6x_4&2x_1+3x_2+3x_3+8x_4\\3x_1+x_2+2x_3+7x_4&3x_1+8x_2+5x_3+7x_4\end{pmatrix}$

也和上面同理

---

# 小结

根据上面的很多例子我们可以得到这样的表格：  
  

$\begin{array}{cccc}\text{输出/输入} & 标量 & 向量 &矩阵 \\ 标量 & 2x+3 & 2x_1+3x_2 & 2x_1+4x_2+6x_3+2x_4\\ 向量 & \begin{pmatrix}2x+1 \\ 3x-1 \end{pmatrix} &\begin{pmatrix}2x_1+2x_2\\3x_1-2x_2\end{pmatrix}& \begin{pmatrix}3x_1+2x_2+5x_3+8x_4\\9x_1-2x_2+4x_3+2x_4\end{pmatrix}\\矩阵& \begin{pmatrix}x+1 &2x+1\\x^2+1&2x^2+2\\x^2+1&3x^3+3\end{pmatrix}& \begin{pmatrix}2x_1+x_2&x_1+7x_2\\6x_1+x_2&2x_1+3x_2\\3x_1+x_2&3x_1+8x_2\end{pmatrix}& \begin{pmatrix}2x_1+x_2+3x_3+2x_4&x_1+7x_2+2x_3+9x+4\\6x_1+x_2+2x_3+6x_4&2x_1+3x_2+3x_3+8x_4\\3x_1+x_2+2x_3+7x_4&3x_1+8x_2+5x_3+7x_4\end{pmatrix}\end{array}$

---

# 矩阵函数求导

## 本质

> 矩阵函数求导的本质是将输出的**函数矩阵**中的每个函数，对输入的**每个元素**（$x_i$﻿）逐个求**偏导**

这样说也还是不够清楚，所以我们直接看例子：

假设我们有这样一个函数

$f(x_1,x_2,x_3) = x_1^2+2x_1x_2+x_3^2$

按照上面所说的，我们对每一个$x_i$﻿求偏导：

$\frac{\partial{f}}{\partial{x_1}}=2x_1+2x_2\\\frac{\partial{f}}{\partial{x_2}}=2x_1\\\frac{\partial{f}}{\partial{x_3}}=2x_3$

然后我们把结果拼起来，就是矩阵函数求导的结果了

$\frac{\partial{f(x)}}{\partial{x}} = \begin{pmatrix}2x_1 +2x_2\\2x_1\\2x_3\end{pmatrix}$

为什么长这样？：这只是一个定义，我们把这个东西定义成了矩阵函数的导数，而且很合理不是吗

为了统一表达形式，这样规定：  
对列向量求导，那么求导结果就是一个列向量  

对行向量求导，那么求导结果就是一个行向量

两个结果互为转置

  

这上面的例子中，我们默认$x$﻿是一个行向量

  

上面这个例子展示的是一个输入向量、输出标量的函数的求导，下面我们来看看更一般的形式

---

## 输入向量输出标量的函数的求导

首先$x = (x_1,\cdots,x_n)^T$﻿是一个列向量

我们有两个求导形式：

行向量求导形式：

$D_xf(x)=\frac{\partial{f(x)}}{\partial{x^T}} = (\frac{\partial{f(x)}}{\partial{x_1}},\cdots,\frac{\partial{f(x)}}{\partial{x_n}})$

列向量求导形式 ： 注意：我们也把这个形式的导数称为**梯度**

$\nabla_x{f(x)}=\frac{\partial{f(x)}}{\partial{x}} = (\frac{\partial{f(x)}}{\partial{x_1}},\cdots,\frac{\partial{f(x)}}{\partial{x_n}})^T$

## 输入矩阵输出标量的函数的求导

我们接下来从向量的导数形式推广到矩阵的导数

输入的矩阵表达为：$x_{m\times n} = \begin{pmatrix}x_{11} & x_{12} & \cdots & x_{1n} \\ \cdots & \cdots & \cdots &\cdots \\ x_{m1} & x_{m2}& \cdots &x_{mn}\end{pmatrix}$﻿

我们有两个方向来表示矩阵导数

---

### 列堆栈形式：

因为矩阵本质上只是向量的升级版，只要我们能知道矩阵的大小（$m\times x$﻿），把矩阵变成向量是没有信息丢失的（我们可以根据大小信息还原矩阵），于是我们考虑一个函数$vec(x)$﻿

$vec(x)$﻿的作用是把矩阵堆栈化，具体操作如下：

![[IMG_0596.jpeg]]

把矩阵处理成向量后，于是我们有：

### 行向量求导形式：

$D_{vec(x)}f(x)=\frac{\partial{f(x)}}{\partial{vec(x)^T}} = (\frac{\partial{f(x)}}{\partial{x_{11}}}, \frac{\partial{f(x)}}{\partial{x_{21}}},\cdots, \frac{\partial{f(x)}}{\partial{x_{m1}}}, \frac{\partial{f(x)}}{\partial{x_{12}}}，\cdots, \frac{\partial{f(x)}}{\partial{x_{mn}}})$

### 列向量求导形式： 这个形式也叫**梯度**

$\nabla{vec(x)}f(x)=\frac{\partial{f(x)}}{\partial{vec(x)}} = (\frac{\partial{f(x)}}{\partial{x_{11}}}, \frac{\partial{f(x)}}{\partial{x_{21}}},\cdots, \frac{\partial{f(x)}}{\partial{x_{m1}}}, \frac{\partial{f(x)}}{\partial{x_{12}}}，\cdots, \frac{\partial{f(x)}}{\partial{x_{mn}}})^T$

---

### 矩阵形式：

我们效仿向量的做法，按元素求导，我们能得到两个形式的矩阵导数

### 雅各比（$Jacobion$﻿）矩阵：

$D_xf(x)=\frac{\partial{f(x)}}{\partial{x_{m\times n}^T}}= \begin{pmatrix}\frac{\partial{f(x)}}{\partial{x_{11}}} & \frac{\partial{f(x)}}{\partial{x_{21}}} & \cdots & \frac{\partial{f(x)}}{\partial{x_{m1}}} \\ \cdots & \cdots & \cdots &\cdots \\ \frac{\partial{f(x)}}{\partial{x_{1n}}} & \frac{\partial{f(x)}}{\partial{x_{2n}}}& \cdots & \frac{\partial{f(x)}}{\partial{x_{mn}}}\end{pmatrix}_{m\times n}$

注意，这里的偏导的排列方式和$x$﻿中的元素的排列方式不同，恰好取了一个**转置**

  

### 梯度矩阵：

$\nabla_x f(x)=\frac{\partial{f(x)}}{\partial{x_{m\times n}}}= \begin{pmatrix}\frac{\partial{f(x)}}{\partial{x_{11}}} & \frac{\partial{f(x)}}{\partial{x_{12}}} & \cdots & \frac{\partial{f(x)}}{\partial{x_{1n}}} \\ \cdots & \cdots & \cdots &\cdots \\ \frac{\partial{f(x)}}{\partial{x_{m1}}} & \frac{\partial{f(x)}}{\partial{x_{m2}}}& \cdots & \frac{\partial{f(x)}}{\partial{x_{mn}}}\end{pmatrix}_{m\times n}$

这里的偏导的排列方式就和$x$﻿中元素的排列方式一样了，这种形式显然更加自然，所以这种形式的使用也是更多的