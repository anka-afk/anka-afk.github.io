# 数值

我们将讨论数值问题,数值解与算法

---

## 数值问题

由一组已知数据(输入),求出一组结果(输出),使得两组数据满足约定的某种关系,这种问题称为**数值问题**

---

## 数值解

由**计算机**计算得到的解称为**数值解**

- 通常而言,数值解是近似值

---

## 数值算法

由给定的已知量经过**有限**次运算得到要知道的数值解,这个计算步骤称为**数值算法**

---

# 误差

用计算机进行数值计算求得的一般都是**近似解**,存在误差

> 误差用于描述数值计算中近似解的**精确程度**

---

## 误差来源

![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled.png|Untitled.png]]

---

### 模型误差

从实际问题转化为数学问题,建立**数学模型**时,我们不可能考虑的面面俱到,这个时候出现的偏差称为**模型误差**

---

### 观测误差

我们的输入数据一般是观测而来的,就像我们的万有引力常数$g$﻿,也是测量得到的,不可避免的会有误差,**测量**中的误差成为**观测误差**

---

### 截断误差

计算机在计算时,必须采取有限次步骤计算完,例如泰勒公式这种有无限项的公式,我们肯定是算不完的,只能取有限项来算,这种在选取计算方法时作出的妥协产生的误差称为**截断误差**或**方法误差**(更加生动)

  

- 例子
    
    ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 1.png|Untitled 1.png]]
    

---

### 舍入误差

因为计算机的内存是有限的，诸如无理数之类的数，计算机只能存储四舍五入后的一部分,这时产生的误差称为**舍入误差**

- 在大量次计算后,微小的舍入误差也会被放大很多,所以要慎重考虑

- 例子
    
    ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 2.png|Untitled 2.png]]
    

---

### 总结

误差是**不可避免**的,要允许误差,控制误差

在数值分析中不讨论模型误差与观测误差,主要讨论截断误差和舍入误差对于大量次计算的影响,以提高**精度**

---

## 绝对误差

### 定义 绝对误差的定义

> 设$x$﻿为精确值,$x^*$﻿为近似值
> 
> 我们定义绝对误差为:
> 
> $e(x^*) = x-x^*$
> 
> 若存在一个正数$\epsilon$﻿,使得
> 
> $\left| e \right| = \left| x^* - x \right| \le \epsilon$
> 
> 我们就称$\epsilon$﻿为**绝对误差限**或**误差限**
> 
> 通常记为:
> 
> $x = x^* \pm \epsilon$
> 
> - 绝对误差可以取**正负**
> - 越**小**越好
> - 不能很好的表示近似值的精确程度(万一数据很大呢)

---

## 相对误差

### 定义 相对误差的定义

> 我们定义相对误差为:
> 
> $e_r(x^*) =\frac{x-x^*}{x}$
> 
> - 多了一个$r$﻿,$relate$﻿
> 
> 若存在正数$\epsilon_r$﻿使得$\left| e_r(x^*) \right| \le \epsilon_r$﻿
> 
> 我们就称$\epsilon_r$﻿为**相对误差限**
> 
>   
> 
> 但是呢,我们很多时候是不知道真值$x^*$﻿的(如果知道我们为什么还要求呢),我们只好**用数值解(近似值)来代替真值:**
> 
> $e_r(x^*) = \frac{x - x^*}{x^*}$
> 
> - 相对误差的大小可以表示近似值的**精确程度**
> - 实际计算中我们得到的是**误差限**或**相对误差限**

---

# 有效数字

## 定义 有效数字的定义

> 若近似值$x^*$﻿的误差限是某一个位的半个单位,且该位到$x^*$﻿的第一位非零数字共有$n$﻿位,则称$x^*$﻿有$n$﻿位有效数字
> 
> 这样说很不直观,看例子:
> 
> - 例子1
>     
>     ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 3.png|Untitled 3.png]]
>     
> - 例子2
>     
>     ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 4.png|Untitled 4.png]]
>     
> 
> ---
> 
> 还有另一种表示:
> 
> 如果我们记$x^* = \pm 0.a_1a_2\cdots a_n \times 10^k$﻿
> 
> 若$\left | x-x^*\right| \le 0.5 \times 10^{k-m}$﻿
> 
> 则$x^*$﻿至少有$n$﻿位有效数字
> 
>   
> 
> 这样其实还是不是很直观,直接看上面的例子就好
> 
> ---
> 
> - 由准确数字进行**四舍五入**得到的近似值都是有效数字
> - 可以通过有效数字的位数确定误差限
> 
> - 例子
>     
>     - 其实就是这个数最小位的半个
>     
>     ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 5.png|Untitled 5.png]]
>     

---

## 定理 有效数字与相对误差的关系

> $n$﻿位有效数字的近似数$x^*$﻿的相对误差具有上界:
> 
> $\left|e_r(x^*)\right|\le \frac{1}{2a_1}\times 10^{-n+1} \quad \quad 且a_1\ne 0(a_1是x^*最左边的非零数字)$
> 
> - 证明
>     
>     ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 6.png|Untitled 6.png]]
>     
> 
> ---
> 
> 反之,若$\left|e_r(x^*)\right|\le \frac{1}{2(a_1+1)}\times 10^{-n+1} \quad \quad 且a_1\ne 0(a_1是x^*最左边的非零数字)$﻿则$x^*$﻿至少有$n$﻿位有效数字
> 
> - 证明
>     
>     ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 7.png|Untitled 7.png]]
>     

- 例子
    
    ![[知识/课内学习/数值分析/数值,误差与算法/pictures/Untitled 8.png|Untitled 8.png]]
    
      
    

---

# 误差估计
我们接下来看看怎么估计我们算法中存在的误差
我们的算法可以看作一个函数, 它接受一些参数, 输出一个结果
那么只要知道怎么估计函数的误差, 就能知道怎么估计算法的误差了

>[!note] 函数误差估计
设 $f(x)$ 是一个一元函数,
函数绝对误差的估计:$$
e(f(x^*))\approx f^{\prime}(x^*)e(x^*)$$
函数相对误差的估计:$$
e_r(f)\approx f^{\prime}(x^*)\frac xfe_r(x)$$
其中 $e(f(x^{*}))=f(x^{*})-f(x)$
相对误差的公式则是由两边除以自身输入的变量得到

推导:
$$\begin{aligned}
e\big (f (x^*)\big)&=f (x)-f (x^*) \\
&=f (x^{*})+f^{\prime}(x^{*})(x-x^{*})+\frac{f^{\prime\prime}(x^{*})}{2}(x-x^{*})^{2}\cdots-f (x^{*}) \\
&=f^{\prime}(x^{*})(x-x^{*})+\frac{f^{\prime\prime}(x^{*})}2 (x-x^{*})^{2}\cdots  \\
&\approx f^{\prime}(x^*)(x-x^*)=f^{\prime}(x^*) e (x^*)
\end{aligned}$$

- 可以看出, 是由泰勒公式推导出来的
- 这也揭示了我们数值算法的本质是逼近 (泰勒公式是多项式逼近)

---
## 二元情形
如果输入函数的变量变成两个, 相应就要做二元的泰勒展开
$$\begin{aligned}
&e(f(x^*,y^*))\approx\frac{\partial f(x^*,y^*)}{\partial x}e(x^*)+\frac{\partial f(x^*,y^*)}{\partial y}e(y^*) \\
&e_{r}(f)\approx\frac{x}{f}\frac{\partial f(x^{*},y^{*})}{\partial x}e_{r}(x^{*})+\frac{y}{f}\frac{\partial f(x^{*},y^{*})}{\partial y}e_{r}(y^{*}) \\

\end{aligned}$$


---
## 推广
$$
\begin{align}
&\bullet e(x\pm y)\approx e(x)\pm e(y) \\
&\bullet e(xy)\approx ye(x)+xe(y) \\
&\bullet e^2(\frac xy)\approx\frac1ye(x)-\frac x{y^2}e(y)  \\
&\bullet e_{r}\left(x\pm y\right)\approx\frac{x}{x\pm y}e_{r}\left(x\right)\pm\frac{y}{x\pm y}e_{r}\left(y\right)
\end{align}
$$
可以发现绝对误差符合<font color="#ff0000">链式法则</font>
和求导的规律一致
而相对误差是<font color="#ff0000">加权运算</font>得到的

---
# 运算时应遵循的原则
为了减小误差, 我们设计算法的时候必须遵循一些原则
## 避免相近的数相减
由于相对误差的积累符合:
$$e_r\left(x\pm y\right)\approx\frac x{x\pm y}e_r\left(x\right)\pm\frac y{x\pm y}e_r\left(y\right)$$
如果是相减, 那么当 $x$ 和 $y$ 很接近的时候, $e_r(x-y)$ 可能就会很大
我们计算机存储数的位数是有限的, 对于浮点数, 如果乘以很大的数, 后面的小数位就会丢失
从而导致有效数位的丢失

>[!example] 例子
>用带有 4 位有效数字的计算器计算 $1-\cos2^{\circ}$
> 1. 直接计算得到: $1-cos2^\circ\approx1-0.9994=0.0006$
> 2. 变换一下计算: $1-cos2°=2sin^21°\approx2\times0.01745^2=0.609\times10^{-3}$
> 3. 再变换一下: $1-cos2^\circ=\frac{sin^22^\circ}{1+cos2^\circ}\approx\frac{0.03490^2}{1+0.9994}\approx0.6092\times10^{-3}$
可以发现, 第三种变换精度是最高的

一般来说, 为了避免相近的数相减, 我们有下面的办法:
$$\begin{aligned}
&\sqrt{x+\varepsilon}-\sqrt{x}= \frac\varepsilon{\sqrt{x+\varepsilon}+\sqrt{x}}  \\
&\ln(x+\varepsilon)-\ln x=\ln\left(1+\frac\varepsilon x\right) \\
&当|x|<<1时: \\
&1-\cos x=2\sin^2\frac x2 \\
&e^x-1= x\Bigg(1+\frac12x+\frac16x^2+...\Bigg) 
\end{aligned}$$
- 基本上都是通过展开来避免这种情况

---
## 避免大数吃小数
什么是大数吃小数呢? 我们之前提到过如果一个很大的数加上一个很小的数, 计算机为了存下这个结果, 由于空间是有限的, 只能把后面的比较小的项给截去了, 这样这个小数就被"吃"了
 由于计算机的字长有限，又要作对阶处理，在数值运算中，如果数据的数量级相差很大，如不注意运算次序，就可能出现大数“吃掉”小数的现象。

>[!example] 例子
若以字长为 4 的计算机计算
$0.1234\times10^5+0.9988\times10^1$ 
$\to\color{red}{0.1234\times10^5+(0.00009988)\times10^5}$
$\to(\color{red}{0.1234+0.0000})\times10^5$ 
> $\to\color{red}{0.1234\times10^5}$

---
## 避免小分母

根据
$$e\left(\frac xy\right)\approx\frac1ye\left(x\right)-\frac x{y^2}e\left(y\right)$$
我们知道, 如果分母 $y$ 非常大, 那么左侧的误差也会非常大

---
## 要减少乘除运算的次数

由于加减法是远远快于乘除法的 (乘除法是很多次加减法), 我们在考虑算法的性能时, 一般只考虑乘除法的次数
- 一个算法所需要的乘法和除法总次数称为计算量。
- 单位为 flop，表示完成一次浮点数乘或除法所需要的时间
- 算法的计算量可以衡量算法的优劣，因为它体现着算法的计算效率，通常算法的计算量越小，则算法的计算效率越高，因而该算法也越好
- 由于计算机做加减法要比乘除法快得多，故算法的计算量可以不考虑加减法的时间

这就需要我们尽量减少乘除法的次数, 尽可能提取公因式, 做完加减法再做乘除法, 具体如下:

>[!example] 例子
>如果计算多项式 $P_4(x)=2x^4+3x^3-2x^2+2x^1–2$
>我们有几种方式:
> 1. 直接计算
>    一共有 $4+3+2+1=10$ 次乘法和 $4$ 次加减法
> 2. 改变形式
>    $\begin{aligned}
P_4 (x)& =2 x^4+3 x^3-2 x^2+2 x-2  \\
&=x (2 x^3+3 x^2-2 x+2)–2 \\
&=x (x (2 x^2+3 x-2)+2)–2 \\
&=x (x (x (2 x+3)-2)+2)-2
\end{aligned}$
只需要 $4$ 次乘法和 $4$ 次加减法

从上面的计算中我们可以发现一个规律, 存在一种很好的方式计算**多项式四则运算**
这样计算计算量会达到最小:
>[!note] 秦九韶法/Horner 法
$$\begin{cases}u_n=a_n\\u_k=xu_{k+1}+a_k,(k=n-1,n-2,\cdots,1,0)\\p(x)=u_0\end{cases}$$
也写作:
$$P_{n}(x)=\{[(a_{n}x+a_{n-1})x+a_{n-2}]x+\cdots+a_{1}\}x+a_{0}$$

---
## 使用数值稳定的算法, 控制误差传播
>[!note] 数值稳定算法
>算法在计算过程中产生的舍入误差不会被累积增大，或者有增长但在一定条件下可控制，从而不会严重降低最终结果的精确度，这种算法称为数值稳定算法。
>反之称为数值不稳定算法。

当然也不是说算法选的好, 误差就会小
有一些问题, 就算算法再好, 一点微小的变化, 结果也会差距很大, 这种问题称为**病态问题**

- 在实际算法的数值稳定性分析中，通常只考虑初始数据误差的影响，不考虑中间过程舍入误差的影响

在一定条件下可以控制, 指的是在计算中, 误差增长不是爆炸式的 (指数式增长)
>[!example] 例子
> 设初始数据的误差为 $\epsilon$, $n$ 步计算以后误差为 $\color{red}{e}$
> 若 $|e_{n}| \approx \color{red}{cn\epsilon}$ (c 与 $\color{red}{n}$ 无关的常数) 即<font color="#ff0000">线性增长</font>
> 可通过 $\epsilon$ 来控制 $e_{n}$, 故算法是稳定的
> 若 $|e_n|\approx\color{red}{k^n\epsilon}$ (k 与 $\color{red}{n}$ 无关的常数，且 $\color{red}{k>1})$ 即<font color="#ff0000">指数增长</font>
无论 $\epsilon$ 多小，$\color{red}{k^n}$ 都有可能使 $\color{red}{e_n}$ 失控，故算法不稳定 

---
# 算法常见思想
- 迭代法
- 逼近
- 离散化
- 加权平均松弛技术
