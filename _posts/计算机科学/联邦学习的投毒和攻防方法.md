---
title: 联邦学习的投毒和攻防方法
date: 2023-10-11 18:59:46
author: xeonds
toc: true
excerpt: (*/ω＼*)
tags:
---
十分的交叉学科。具体而言就是机器学习领域的数据安全问题。

首先是标题 说的大概是一种新的人工智能学习范式。在这种防方式里也有攻击方式和相应的防御方式。

## 联邦学习简介


- definition 联邦学习：一种分布式学习方法。解决的问题主要针对数据量不够/数据获取途径（成本问题），数据难以集中(法律问题和隐私问题)。一种时间方法是在本地进行小规模训练，然后将训练得到的模型参数上传汇总。不但规避了隐私版权问题，也保证了训练效果。

根据以样本和特征分别作为基准，联邦学习又分为横向/纵向：同特征不同样本/同样本不同特征。它们各自有着相当不同的模型参数聚合方法。后者通常发生在大型企业之间，而前者通常发生在用户/公司之间，双方并非信任关系。

前者的聚合方法有简单的加权平均FedAvg，有离群点修复FedProx，还有SCAFFOLD等。而后者则利用隐私求交集PSI来完成加密实体对齐。典型的纵向联邦学习框架有SecureBoost。

但是数据特征仍然是反映在梯度参数中的，通过深度梯度泄漏方法DLG，可以恢复训练数据，这样就间接泄漏了训练数据的隐私。

## 隐私数据保护方法

防止梯度泄露的方法主要是两类：密码学方法和差分隐私（Differential Privacy）.前者有MPC和HE等，通信开销和计算量较大，但是精度较高。后者则有噪声问题的困扰。

## 联邦学习的攻击方法

目的就是使训练模型很难收敛，破坏联邦学习模型的性能和可信度。

- 投毒攻击，给本地数据添加错误，来弱化学习效果；或者更改分布式训练的模型参数。攻击者的知识、能力越强则攻击能力越强

但是太离谱的数据会被数据清洗过滤掉，太小的数据又没啥效果。

- 目标/非目标攻击：针对特定/随即目标进行数据编造投毒攻击。

- 数据打乱：比如对于不同群体的用户数据，互换他们attribute的结果，最终使得结果在不同群体上相反。

## 投毒攻击的检测

这种检测难度不小：投毒攻击检测和隐私保护间的矛盾、数据非独立分布、攻击隐蔽性。

比较朴素的思路就是使用数据挖掘的清洗方法，对离群点等进行清洗。比如说有一些聚合方法：Krum, Bulyan, Trimmed mean（修整均值）, Medium（中位数）等，都是一些经典的鲁棒性聚合规则。

## 防御方法

首先是基于统计学的方法。但是它的局限性是只有少部分客户端被控制。

其次是用验证用数据集进行验证。不过这样纯粹的数据集很难找。

再者就是用机器学习方法1-AE进行检验。它首先在本地预训练一个自编码器模型，然后正式训练过程中基于自编码器计算各节点模型更新的重构误差。

2-GAN：可以用GAN尝试恢复出来训练样本来生成本地训练集，从而验证模型参数是否正常。但是缺陷是，无法应对在训练初期就发动攻击的方式。

区块链1-FLChain：由部分节点进行数据检测再进行上传，从而保证参数的正确性。问题是这部分节点如何选择，以及是否正常。解决方法可以是动态更新这部分成员结点，通过一系列置信度去更新这部分节点。

## 研究成果：神经网络模型水印

初始模型嵌入水印，并对水印进行检测。缺点是上传的模型不能加密。当然也可以允许加密，不过可以聚合模型，用聚合后的模型检验上传者。或者，可以比对上传者的历史行为数据，以此检测上传节点是否异常。

时间有限，基本就到这里了。

