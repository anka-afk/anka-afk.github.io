{"posts":[{"title":"Gauss-Seidel迭代法","text":"#定义也称高斯-赛德尔迭代法观察 $Jacobi$ 迭代法, 我们发现, 在求 $\\color{red}{x^{(k+1)}}\\text{ }$ 的分量时,当求到第 $i$ 个分量: $\\color{red}{x_i^{(k+1)}}$ 时, 我们已经求到了 $x_{1}^{(k+1)},x_{2}^{(k+1)}\\cdots x_{i-1}^{(k+1)}$我们知道, $x_{1}^{(k+1)},x_{2}^{(k+1)}\\cdots x_{i-1}^{(k+1)}$ 理论上是比 $x_{1}^{(k)},x_{2}^{(k)}\\cdots x_{i-1}^{(k)}$ 要接近真值的,我们不禁想到, 如果用 $x_{1}^{(k+1)},x_{2}^{(k+1)}\\cdots x_{i-1}^{(k+1)}$ 代替 $x_{1}^{(k)},x_{2}^{(k)}\\cdots x_{i-1}^{(k)}$会不会使得迭代法收敛更快呢?(毕竟使用了更加接近真值的信息) 如果将 Jacobi 迭代法得到的解立刻投入到下一次运算中, 我们得到高斯-赛德尔迭代法 基于上面的想法, 我们对 $Jacobi$ 方法进行改造得到 $\\begin{cases}\\mathrm{x}1^{(k+1)}=\\frac1{a{11}}(-a_{12}\\mathrm{x}2^{(k)}-a{13}\\mathrm{x}3^{(k)}-a{14}\\mathrm{x}4^{(k)}-\\cdots-a{1n}\\mathrm{x}_n^{(k)}+b_1)\\\\\\mathrm{x}2^{(k+1)}=\\frac1{a{22}}(-a_{21}\\mathrm{x}1^{(k+1)}-a{23}\\mathrm{x}3^{(k)}-a{24}\\mathrm{x}4^{(k)}-\\cdots-a{2n}\\mathrm{x}n^{(k)}+b_2)\\\\\\mathrm{~x}3^{(k+1)}=\\frac1{a{33}}(-a{31}\\mathrm{x}1^{(k+1)}-a{32}\\mathrm{x}2^{(k+1)}-a{34}\\mathrm{x}4^{(k)}-\\cdots-a{3n}\\mathrm{x}n^{(k)}+b_3)\\\\\\cdots&amp;\\cdots&amp;\\cdots\\\\\\mathrm{~x}n^{(k+1)}=\\frac1{a_m}\\left(-a{n1}\\mathrm{x}1^{(k+1)}-a{n2}\\mathrm{x}2^{(k+1)}-a{n2}\\mathrm{x}3^{(k+1)}-\\cdots-a{m-1}\\mathrm{x}{n-1}^{(k+1)}+b_n\\right)&amp;\\end{cases}$$x_i^{(k)}=\\frac{1}{a_{ii}}\\left[-\\sum_{j=1}^{i-1}(a_{ij}x_j^{(k)})-\\sum_{j=i+1}^{n}(a_{ij}x_j^{(k-1)})+b_i\\right],$ 看起来确实很丑, 但是推导是容易的我们来看比较好看的矩阵形式: $\\begin{aligned}&amp;x^{(k+1)} = -D^{-1}(Lx^{(k+1)} +Ux^{(k)}) + D^{-1}b \\ &amp;\\Rightarrow x^{(k=1)}= -(D+L)^{-1}Ux^{(k)}+(D+L)^{-1}b\\end{aligned}$ 推导是很直观的, $x_{i}^{(k+1)}$ 的系数都是矩阵 $A$ 的下三角部分而 $x_{i}^{(k)}$ 的系数都是矩阵的上三角部分 特点#性质 这个迭代法的想法是, $x^{(k+1)}$ 是比 $x^{(k)}$ 更加接近精确解 $x^*$ 的, 我们就直接利用当前算出的一些 $x^{(k+1)}$ 的分量计算其他分量, 例如算 $x^{(k+1)}_2$ 时, 我们已经算出 $x^{(k+1)}_1$ 了, 我们可以用这个代替 $x^{k}_1$ 来计算 $x^{(k+1)}_2$ , 因为 $x_1^{(k+1)}$ 更加接近精确解的第一个分量. 因为我们是根据当前步骤已经算出的分量算其他的分量, 所以这个方法的解的分量 $x_i$ 有先后之分 一般来说这个方法比 Jacobi 迭代法收敛更快 例子#例子$\\begin{cases}10x_1-2x_2-x_3=3\\-2x_1+10x_2-x_3=15\\-x_1-2x_2+5x_3=10&amp;\\end{cases}$经过高斯-赛德尔迭代法得到的迭代公式:$$\\begin{aligned}\\left|x_1^{(k+1)}\\right.&amp;=\\frac 1{10}(2{x_2}^{(k)}+{x_3}^{(k)}+3) \\\\left|x_2^{(k+1)}\\right.&amp;=\\frac 1{10}(2{x_1}^{(k+1)}+{x_3}^{(k)}+15) \\\\left|x_3^{(k+1)}\\right.&amp; =\\frac 15 ({x_1}^{(k+1)}+2{x_2}^{(k+1)}+10)\\end{aligned}$$ 代码$\\varepsilon$ 为精度要求，$k$ 记录迭代次数，$N$ 为允许的最大迭代次数，二维数组 ${a_{ij}}{i,j=1}^{n+1}$ 存放 $A,b$. 一维数组 ${x_i}{i=1}^n$ 开始存放初值，迭代过程存放迭代值。 $\\color{red}{k=0}$ 对 $\\color{red}{i=1,2\\cdots n}$, 置初值 $x_i=0$ $\\color{red}{k=k+1}$ 若 $\\color{red}{k&gt;N}$, 则输出当前迭代值 $x_{i}$ 和 $k$, 停机 $\\quad P_0=0$ 对 $\\color{red}{i=1,2\\cdots n}$$P\\leftarrow\\Delta x_i=(a_{i,n+1}-\\sum_{j=1}^{n}\\alpha_{ij}x_j)/\\alpha_{ii}^{}$若 $|P_0|&lt;|P|$, 则 $P_0=P$$\\color{red}{x_i=x_i+P}$ 如果 $\\color{red}{|P_0|&lt;\\varepsilon}$, 则进入步骤 8，否则转入步骤3 输出近似解 $\\quad\\text{}\\color{red}{x_i(i=1,2\\cdots n)}$ 和 $k$","link":"/2024/04/17/Gauss-Seidel%E8%BF%AD%E4%BB%A3%E6%B3%95/"},{"title":"jacobi迭代法","text":"#定义 把系数矩阵写为 $A=D+L+U$ 的形式 其中 $D$ 为 $A$ 的主对角线组成的对角矩阵 $L$ 为 $A$ 的严格下三角部分组合的严格下三角矩阵(不含对角线) $U$ 为 $A$ 的严格上三角部分组合的严格上三角矩阵(不含对角线) 我们把迭代表达式写为: $x^{(k+1)} = -D^{-1}(L+U)x^{(k)} + D^{-1}b$ 我们称这种迭代方法为Jacobi 迭代法 怎么构造 Jacobi 迭代公式由方程组 $AX = b$ 的第 i 个方程解出 $X_i\\left (i=1,2,\\cdots, n\\right)$ 得到一个同解方程组 $\\begin{cases}x_1=\\dfrac{1}{a_{11}}(-a_{12}x_2-a_{13}x_3-\\cdots-a_{1n}x_n+b_1)\\[2ex]x_2=\\dfrac{1}{a_2}(-a_{21}x_1-a_{23}x_3-\\cdots-a_{2n}x_n+b_2)\\[2ex]……\\x_n=\\dfrac{1}{a_{n1}}(-a_{n1}x_1-a_{n2}x_2-\\cdots-a_{n,n-1}x_{n-1}+b_n)\\end{cases}$ 我们修改一下右边的 $x_{i}$, 得到迭代公式: $\\begin{cases}x_{1}^{(k+1)}=\\frac{1}{a_{11}}(-a_{12}x_{2}^{(k)}-a_{13}x_{3}^{(k)}-\\cdots-a_{1n}x_{n}^{(k)}+b_{1})\\\\x_{2}^{(k+1)}=\\frac{1}{a_{22}}\\big(-a_{21}x_{1}^{(k)}-a_{23}x_{3}^{(k)}-\\cdots-a_{2n}x_{n}^{(k)}+b_{2}\\big)\\\\cdots\\cdots\\x_{n}^{(k+1)}=\\frac{1}{a_{nm}}\\big(-a_{n1}x_{1}^{(k)}-a_{n2}x_{2}^{(k)}-\\cdots-a_{n,n-1}x_{n-1}^{(k)}+b_{n}\\big)\\end{cases}$ 我们有了迭代公式, 总得有一个起点吧#符号我们找一个初始向量: $X^{(0)}=(x_1^{(0)},x_2^{(0)},\\cdots,x_n^{(0)})^T$ 上面的公式就称为 $Jacobi$ 迭代公式 矩阵形式#符号我们引入三个记号: 我们用 $D$ 记矩阵 $A$ 的对角线部分: $D=\\begin{bmatrix}a_{11}&amp;&amp;&amp;\\&amp;a_{22}&amp;&amp;\\&amp;&amp;\\ddots&amp;\\&amp;&amp;&amp;a_{nn}\\end{bmatrix}$我们用 $L$ 记矩阵 $A_{}$ 的下三角部分(不包括对角线) :$L=\\begin{bmatrix}\\mathbf{0}\\a_{21}&amp;\\mathbf{0}\\\\cdots&amp;\\cdots\\a_{n1}&amp;a_{n2}&amp;\\cdots&amp;\\mathbf{0}\\end{bmatrix}$我们用 $U_{}$ 记矩阵 $A$ 的上三角部分 (不包括对角线) :$U=\\begin{bmatrix}0&amp;a_{12}&amp;\\cdots&amp;a_{1n}\\&amp;0&amp;\\cdots&amp;a_{2n}\\&amp;&amp;&amp;\\vdots\\&amp;&amp;&amp;\\mathbf{0}\\end{bmatrix}$ 则有 $A{=}D{+}L{+}U$ 成立，矩阵形式为 $${Dx=-(L+U)x+b}$$等式两边乘以 $D^{-1}$, 得 $x=-D^{-1}(L+U)$ $x+D^{-1}b$ #定义于是我们得到迭代公式:$x^{(k+1)}=-D^{-1}{(L+U)}x^{(k)}+D^{-1}b$ 如果不用矩阵形式, 还可以表示为: $\\begin{aligned}x^{(0)} = (x_1^{(0)},x_2^{(0)},\\cdots,x_n^{(0)})^T \\ x_i^{(k+1)} = (b_i-\\sum_{j=1,j\\ne i }^na_ijx_j^{(k)})/a_{ii} \\ i=1,2,\\cdots,n \\quad k = 0,1,2,\\cdots \\quad k表示迭代次数\\end{aligned}$ 特点#性质 $\\text{迭代矩阵 }B_J=-D^{-1}(L+U)$ 每迭代一次主要是计算一次矩阵乘向量 $B_Jx^{(k)}$ 所以计算量为 $n^2$ 数量级。 计算过程中涉及到的中间变量 $x^{(k)}$ 及 $x^{(k+1)}$,需要两组工作单元 $x_{n}$, $y_{n}$来存储} $\\text{计算过程中,初始数据A始终不变;}$ 解的分量 $ｘ_i$ 没有先后之分, 适合并行运算 公式简单, 迭代矩阵容易求到 也称同事替换法(相较于后面的方法) 迭代终止条件$\\text{若迭代矩阵}\\mathsf{B}\\text{满足}|B|&lt;1\\text{则}$$\\left|X^*-X^{(k)}\\right|\\leq\\frac{\\left|B\\right|}{1-\\left|B\\right|}\\left|X^{(k)}-X^{(k-1)}\\right|$ 这个公式给出了误差的估计, 也就是迭代点和真值的差距可以用两个相邻迭代点的差距约束于是我们有一个停止迭代的条件 (终止条件)$$\\left|X^{(k-1)}- X^{(k-1)}\\right|&lt;\\varepsilon $$ 例子#例子$\\begin{cases}10x_1-2x_2-x_3=3\\-2x_1+10x_2-x_3=15\\-x_1-2x_2+5x_3=10&amp;\\end{cases}$求 $Jacobi$ 迭代公式就是第 $i$ 行把其他元素都移到右边, 左边剩 $x_{i}$, 然后把左边的变成 $x_{i}^{(k+1)}$, 右边的变成 $x_{i}^{(k)}$得到 $$\\begin{gathered}x_{1}^{(k+1)}=\\frac{1}{10}(2{x_2}^{(k)}+{x_3}^{(k)}+3) \\x_{2}^{(k+1)}=\\frac1{10}(2{x_1}^{(k)}+{x_3}^{(k)}+15) \\x_{3}^{(k+1)}=\\frac15{(x_1^{(k)}+2x_2^{(k)}+10)}\\end{gathered}$$ 代码#代码 123456789101112function y=jacobi(A,b,x,M)D = diag(diag(A));U = -triu(A,1)L = -tril(A,-1);B = D\\(L+U)f= D\\b;y = B*x+f ; n=1:while norm(y-x)&gt;=1.0e-6&amp;n&lt;Mx= yy=B*x+f;n=n+1;end","link":"/2024/04/17/Jacobi%E8%BF%AD%E4%BB%A3%E6%B3%95/"},{"title":"Rust学习","text":"参考视频：【Rust 编程语言入门教程（Rust 语言/Rust 权威指南配套）【已完结】】 https://www.bilibili.com/video/BV1hp4y1k7SV/?p=12&amp;share_source=copy_web&amp;vd_source=faa4d83326aa65fa16123ce1d49425a3 我将采用注释式笔记, 也就是所有知识都用代码文件存放, 在注释中写明知识点 [[猜数游戏]][[rust变量类型]]","link":"/2024/04/17/Rust/"},{"title":"Rust变量类型","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293fn main() { println!(&quot;Hello, world!&quot;); // 不可变变量 let mut x = 5; println!(&quot;x的值是:{}&quot;,x); x = 6; println!(&quot;x的值是:{}&quot;,x); // 常量也是不可变的,但是不可约使用mut,永远都不可变 // 声明常量必须使用const // 声明常量必须标注类型 // 常量可以在任何作用域声明,包括全局作用域 // 常量只能被设置为常量表达式,不能是函数调用的结果,或任何其他只能在运行时计算得到的值 // 常量在运行期间总是存在的,并且它们在任何作用域中都可以被访问 // 常量使用全大写字母和下划线命名,并且下划线可以用来增加可读性 // 数字中间可以插入下划线来增加可读性 const MAX_POINTS:u32 = 100_000; // 隐藏变量 // 可以用同样的名字声明一个新变量,而新变量会隐藏之前的变量 let x = 5; println!(&quot;x的值是:{}&quot;,x); let x = x + 1; println!(&quot;x的值是:{}&quot;,x); // 虽然x是不可变的,但是可以用let关键字来声明一个新变量,这样就可以隐藏之前的变量 // 相当于override let x = x * 2; println!(&quot;x的值是:{}&quot;,x); // shadow和把变量标记为mut是不一样的 // 用let声明的新变量也是不可变的 // 使用let声明的同名新变量的类型可以和之前的不一样 let spaces = &quot; &quot;; let spaces = spaces.len(); // 这里把spaces的类型从字符串变成了数字,但是名字还是spaces,其实是一个新的变量,同名隐藏掉原来的变量 // 如果这里不加let,会报错,因为变量是不可变的 println!(&quot;spaces的值是:{}&quot;,spaces); // 数据类型 // Rust是静态类型语言,必须在编译时知道所有变量的类型 // 如果可能的类型比较多,例如parse方法,就必须添加类型的标注 let guess:u32 = &quot;43&quot;.parse().expect(&quot;不是个数!&quot;); // 如果不标注是u32类型,会报错 println!(&quot;guess的值是:{}&quot;,guess); // 标量类型 // 标量类型代表单个的值 // rust一共有四个主要的标量类型:整数,浮点数,布尔类型,字符类型 // 整数类型 // 有符号整数:i8,i16,i32,i64,i128,isize // 无符号整数:u8,u16,u32,u64,u128,usize // isize和usize类型依赖运行程序的计算机架构:64位计算机上是64位,32位计算机上是32位 // 默认是i32类型 // 除了byte类型,其他类型都可以用类型后缀来标注,例如123u32 // 整数溢出 // Rust在编译时会检查是否有整数溢出,如果有,会报错 // debug模式下会panic,release模式下会wrap,例如256u8会变成0,257u8会变成1 // 例如:let x:u8 = 256;会报错 // 浮点数类型 // rust有两种浮点数类型:f32和f64 // 默认是f64类型 let x = 2.0; let y:f32 = 3.0; // 数值操作 // 加法 let sum = 5 + 10; let difference = 95.5 - 4.3; let producy = 4 * 30; let quotient = 56.7 / 32.2; let reminder = 54 % 5; // 布尔类型 // rust有两个布尔值:true和false // 布尔类型的大小是1个字节 // 布尔类型的值是bool类型 let t = true; let f:bool = false; // 字符类型 // Rust的char类型是单个Unicode字符,可以表示比ASCII更多的字符,例如中文,日文,韩文等,甚至是emoji表情 // Rust的char类型是4个字节 // Rust的char类型用单引号表示 // Unicode中没有字符的概念,只有代码点,代码点是一个数字,代表一个字符 let x = 'z'; let y:char = '😻'; let z = '中'; println!(&quot;x的值是:{}&quot;,x); println!(&quot;y的值是:{}&quot;,y); println!(&quot;z的值是:{}&quot;,z); // 复合类型 // Rust有两个复合类型:元组和数组 // 元组可以将多个不同类型的值组合在一起 // 数组可以将多个相同类型的值组合在一起 // 元组tuple // 元组是一个将多个其他类型的值组合在一起的复合类型 // 元组的长度是固定的,一旦声明,就不能增加或减少 let tup:(i32,f64,u8) = (500,6.4,1); // 获取tuple的值 // 可以使用模式匹配来解构一个tuple来获取元素的值 let (x,y,z) = tup; println!(&quot;x的值是:{}&quot;,x); println!(&quot;y的值是:{}&quot;,y); println!(&quot;z的值是:{}&quot;,z); // 访问tuple的元素 // 使用点号和索引来访问tuple的元素 println!(&quot;{},{},{}&quot;,tup.0,tup.1,tup.2); // 数组 // 数组是一个将多个相同类型的值组合在一起的复合类型 // 数组的长度是固定的,一旦声明,就不能增加或减少 // 数组的类型标注是[Type;size],例如:[i32;5] // 数组的元素可以通过索引访问,索引从0开始 let a = [1,2,3,4,5]; let b:[char;5] = ['a','b','c','d','e']; // 如果想把数据存在stack(栈)上而不是heap(堆)上就可以使用数组 // 如果想保证有固定数量的元素,也可以使用数组 // 数组没有Vector灵活 // Vector是可以改变长度的,数组是固定长度的 let months = [&quot;January&quot;,&quot;February&quot;,&quot;March&quot;,&quot;April&quot;,&quot;May&quot;,&quot;June&quot;,&quot;July&quot;,&quot;August&quot;,&quot;September&quot;,&quot;October&quot;,&quot;November&quot;,&quot;December&quot;]; // 这种不会变长度的就适合用数组 // 如果每个元素都相同,还有一种语法 let a = [3;5]; // 这样就会生成一个包含5个3的数组 // 访问数组元素 // 数组是栈上的单个块的内存 let first = months[0]; let second = months[1]; println!(&quot;first的值是:{}&quot;,first); println!(&quot;second的值是:{}&quot;,second); // 如果访问的索引超过了数组的范围,会报错panic // rust不允许其继续访问相应地址的内存,这样可以避免潜在的安全漏洞 // 不允许越界,只能访问数组占的内存空间 // 例如:let index = 10;let element = a[index];会报错 // 有时不会报错,例如:let index = [12,13,14,15]; let month = months[index[1]];可以通过编译,但是运行时会报错index out of bounds,程序panic}","link":"/2024/04/17/rust%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B/"},{"title":"Rust猜数游戏","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104use std::io; // prelude 序曲use rand::Rng; // trait 是其他语言的接口,上面有很多方法use std::cmp::Ordering; // 是一个枚举类型,有三个值,大于,小于,和等于// rust的库称为crate,crate是一个二进制或库,库是一个包含函数和其他代码的包,二进制crate是一个可以被执行的包,每个crate都有一个crate root,这是一个源文件,编译器从这个文件开始并编译整个crate,crate root是一个源文件,而不是一个目录,一个crate可以包含多个库,但只能包含一个二进制crate// crate分为二进制crate还有lib库,二进制crate是一个可以被执行的包,lib库是一个包含函数和其他代码的包,但不能被执行,只能被其他crate引用 fn main() { println!(&quot;猜数!&quot;); // rng是个接口,上面一个函数gen_range,范围是1-100,包括1,不包括101 let secret_number = rand::thread_rng().gen_range(1..101); // println!(&quot;神秘数字是:{}!&quot;, secret_number); // 使用loop循环,无限循环,直到猜对为止 loop{ println!(&quot;猜测一个数:&quot;); // let mut foo = 1; // let bar = foo; // immutable // 要表示0到100的范围,使用u32,无符号32位整数,默认是i32,有符号32位整数,也可以i64 let mut guess = String::new(); // 产生Stdin实例,存句柄,用read_line方法用可变的字符串参数获取输入,前面&amp;表示是引用传递参数,引用默认是不可变的,需要加入mut参数,readline返回Result类型变量,是枚举类型,值称为变体,两个变体:ok和err,ok变体包含一个元组,元组的第一个元素是一个引用,指向一个字符串,这个字符串是readline方法读取的内容,err变体包含一个错误值 // Result上还定义了方法expect,如果是ok,则这个方法会提取里面的值,如果返回err,则expect会使程序崩溃,并显示传递给expect的参数,显示失败原因 // 不调用expect方法也行,但是会出现警告,安全性保证 io::stdin().read_line(&amp;mut guess) .expect(&quot;无法读取行&quot;); // rust允许使用同名的新变量来隐藏之前的变量 // trim方法去掉字符串前后的空白字符,parse方法将字符串转换为数字,parse方法返回Result类型,所以需要expect方法 let guess:u32 = match guess.trim().parse(){ Ok(num) =&gt; num, Err(_) =&gt; continue, }; // parse方法将字符串转换为数字,parse方法返回Result类型,所以需要expect方法 // 声明u32是为了告诉parse()方法,我们想要一个u32类型的数字,如果不声明,parse()方法会返回一个通用数字类型,这样就需要告诉编译器我们想要的类型 // 这之后的guess是一个新的变量,是一个u32类型,之前的guess是一个字符串类型 // 花括号是后面变量的值,用来在字符串中插入变量值 println!(&quot;你猜测的数是: {}&quot;, guess); // 方法guess.cmp,compare,三种情况,使用一个枚举类型 // match表达式,根据后面返回值决定做什么,多个手臂arm组成,每个手臂都有一个匹配的模式,匹配上就执行操作 // 如果match紧跟着的某个值和某个手臂的模式匹配,就会执行这个手臂的代码块 // 静态强类型语言 match guess.cmp(&amp;secret_number){ Ordering::Less =&gt; println!(&quot;太小了!&quot;), // arm Ordering::Greater =&gt; println!(&quot;太大了!&quot;), Ordering::Equal =&gt; { println!(&quot;猜对啦!&quot;); break; }, } }}","link":"/2024/04/17/%E7%8C%9C%E6%95%B0%E6%B8%B8%E6%88%8F/"},{"title":"矩阵分解与高斯消元","text":"我们之前就是消元法解方程嘛，先联立几个方程，然后把变量分离出来代入，最后得到一行只有一个变量的方程，把那个变量解出来，再重复上面的过程就可以了但是计算机是不会干这件事的，这件事对于计算机来说，还是比较难的我们要把这个问题变得简单一点，交给计算机去解决 三角形矩阵回代求解三角形矩阵什么样的方程组好解呢? 直观地看, 三角形的方程组应该是好解的$$\\begin{cases}a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n=b_1\\a_{22}x_2+\\cdots+a_{2n}x_n=b_2\\\\cdots\\cdots\\a_{nn}x_n=b_n\\end{cases}$$像这个方程组, 我们只要从最下面一行开始解, 解出 $x_n$ 再往上代入就行了重复上面的操作, 我们就可以解出所有的解$$\\begin{cases}&amp;x_n=b_n/a_{nn}\\&amp;x_k=\\frac{b_k-\\sum_{j=k+1}^na_{kj}x_j}{a_{kk}}&amp;(k=n-1,n-2,\\cdots,1)\\end{cases}$$ 这个过程称为回代过程 计算量为 $1+2+\\cdots+n=n(n+1)/2$, 为平方级 同理对下三角矩阵:$$\\begin{pmatrix}l_{11}\\l_{21}&amp;l_{22}\\\\vdots&amp;\\vdots&amp;\\ddots\\l_{n1}&amp;l_{n2}&amp;\\cdots&amp;l_{nn}\\end{pmatrix}\\begin{pmatrix}x_1\\x_2\\\\vdots\\x_n\\end{pmatrix}=\\begin{pmatrix}b_1\\b_2\\\\vdots\\b_n\\end{pmatrix}$$容易求得解:$$\\begin{aligned}&amp;x_1=\\frac{b_1}{l_{11}}\\&amp;x_i=\\frac{b_i-\\sum_{j=1}^{i-1}l_{ij}x_j}{l_{ii}} i=2,3,\\cdots,n\\end{aligned}$$ 我们在[[高等代数]]中学习过, 通过相似变换, 线性方程组的解是不会变的于是我们想到, 能不能通过相似变换, 把一个线性方程组 (矩阵) 变换成三角形的呢?#符号 从此开始, 我们把解线性方程组同解矩阵等同起来 如何将矩阵化为三角形把矩阵变成上/下三角形, 我们主要有两种办法 高斯消元法 非奇异矩阵三角分解 高斯消元法与矩阵分解我们一直在用的解线性方程组的办法, 叫做高斯消元法[[高斯消元法例子]]由上面的例子可以看出, 高斯消元法包括两个过程: 消元和回代于是我们有如下方法 顺序高斯消元法 [!note] 顺序高斯消元考虑一个 $n$ 阶线性方程组$$\\begin{cases}a_{11}x_1+a_{12}x_2+…+a_{1n}x_n=b_1\\a_{21}x_1+a_{22}x_2+…+a_{2n}x_n=b_2\\\\vdots\\a_{n1}x_1+a_{n2}x_2+…+a_{nn}x_n=b_n&amp;\\end{cases}$$把它化为矩阵形式:$$Ax=b$$于是我们对矩阵进行操作顺序高斯消元法的主要思路是, 把系数矩阵 $A$ 变成上三角矩阵, 然后回代求解![[Pasted image 20240413233208.png|300]]其实就是:$$\\begin{aligned}&amp;x_n=b_n^{(n)}/a_{nn}^{(n)}\\&amp;x_i=\\left(b_i^{(i)}-\\sum_{j=i+1}^na_{ij}^{(i)}x_j\\right)/a_{ii}^{(i)}\\end{aligned}$$ 顺序高斯消元法硬性要求: 主元全都不为 $0$ 主元即矩阵对角线上的元素: $a_{ii}^{(i)}\\left(i=1,2,…,n\\right)$ 顺序高斯消元法的条件上面我们说过, 顺序高斯消元法要求主元 (对角元) 全都不为 $0$这是因为我们的公式里有 $b_n^{(n)}/a_{nn}^{(n)}$, 分母是不能为 $0$ 的 那么什么时候主元全都不为 $0$ 呢?下面给出一个定理 [!note] 主元不为 0 的充要条件$a_{ii}^{(i)}\\neq0(i=1,\\quad2,\\cdots n)$, 的充要条件是 $A$ 的顺序主子式不为零，即$$D_1=a_{11}\\neq0,\\quad D_i=\\begin{vmatrix}a_{11}&amp;\\cdots&amp;a_{1i}\\\\vdots&amp;\\ddots&amp;\\vdots\\a_{i1}&amp;\\cdots&amp;a_{ii}\\end{vmatrix}\\neq0,\\quad i=1,2,…,n$$ 具体操作[[顺序高斯消元法具体操作]]对增广矩阵 $(\\mathbf A,\\mathbf b)$ 进行初等行变换，直到化成 阶梯矩阵 ，然后回代计算出解 假设在消去过程中主对角元素始终不为 0不断利用每行主对角元素消去下面同列的元素，逐渐使矩阵变成阶梯矩阵（上三角矩阵）完成后回代求解 计算量![[Pasted image 20240413234759.png|325]] 优缺点优点: 计算量较小，结果准确 简单易行 缺点: 如果某个主元为 $0$, 就不能进行 如果某个主元很小, 精度会大大降低 ([[数值,误差与算法#避免小分母|小分母]]或者[[数值,误差与算法#避免大数吃小数|大数吃小数]])(也称为淹没) 代码123456789101112131415161718192021function [ X, U ] = GaussElim( A, b )M = [A b];%增广矩阵[n, ~] = size(A);X = zeros(n,1);%消去for i = 1:n-1%消去for j = i+1:n%计算得到乘子:mm = M(j,i)/M(i,i);M(j,:) = M(j,:)-m*M(i,:);endendU = M(:,(1:n));%回代过程bn = M(:,n+1);X(n) = bn(n)/M(n,n);for k = 1:n-1X(n-k) = (bn(n-k)-M(n-k,(n-k+1:n))*X(n-k+1:n))/(n-k);endend LU 分解(Doolittle) 也称为Doolittle分解接下来我们将说明一个事实 ,就是顺序高斯消元法的消元过程等价于矩阵的 LU 分解什么是 LU 分解? [!note] LU 分解如果我们有一个矩阵 $A$, 把它分解为一个单位下三角矩阵 $L$ 和一个上三角矩阵 $U$即$$A = LU$$这个过程称为矩阵 $A$ 的 $LU$ 分解 $L$ 是一个单位下三角矩阵, 它的对角线都是 $1$ 推导[[LU分解对应高斯消元法例子]] LU 分解回代过程既然我们把高斯消去法的消去步骤表示为 LU 分解, 那么如何转化回代步骤?一旦知道 $L$ 和 $U$, 问题 $Ax = b$ 就可以写成 $LUx=b$, 我们定义辅助向量 $c=Ux$那么回代过程就变成了两步: 对于方程 $Lc=b$, 求解 $c.$ 对于方程 $Ux=c:$, 求解 $x.$由于 $L$ 和 $U$ 都是三角形的矩阵, 两步运算都很直接例如:$$\\begin{bmatrix}1&amp;1\\3&amp;-4\\end{bmatrix}=LU=\\begin{bmatrix}1&amp;0\\3&amp;1\\end{bmatrix}\\begin{bmatrix}1&amp;1\\0&amp;-7\\end{bmatrix}$$先进行步骤 1$$\\begin{bmatrix}1&amp;&amp;0\\3&amp;&amp;1\\end{bmatrix}\\begin{bmatrix}c_1\\c_2\\end{bmatrix}=\\begin{bmatrix}3\\2\\end{bmatrix}$$$$\\begin{align}c_1+0c_2=3\\3c_1+c_2=2\\end{align}$$解得 $c_{1}=3,c_{2}=-7$再进行步骤 2$$\\begin{bmatrix}1&amp;&amp;1\\0&amp;&amp;-7\\end{bmatrix}\\begin{bmatrix}x_1\\x_2\\end{bmatrix}=\\begin{bmatrix}&amp;3\\-7\\end{bmatrix}$$$$\\begin{aligned}x_1+x_2&amp;=3\\-7x_2&amp;=-7\\end{aligned}$$解得 $x_{2}=1,x_{1}=2$ LU 分解的条件我们上面说到, 矩阵的 $LU$ 分解是等价于顺序高斯消元法的, 这意味着两者的条件是相同的$$\\text{LU分解存在} \\Leftrightarrow \\text{高斯消元法不中断} \\Leftrightarrow a_{kk}^{(k)} \\ne 0 \\Leftrightarrow \\text{所有顺序主子式不为}0$$参考[[直接法#顺序高斯消元法的条件|顺序高斯消元法的条件]] 计算量计算量和顺序高斯消元法一致 优缺点 和高斯消元法基本一致, 但是, 如果等式的右边的项 $b$ 替换后,$LU$ 分解不用重新进行计算, 相比高斯消元法更好 对于三对角或者对称正定线性方程组, $LU$ 分解更加高效 代码12345678910111213141516171819202122232425function [ X,L, U ] = GaussElimLU( A, b )M = [A b];%增广矩阵[n, ~] = size(A);X = zeros(n,1);L = eye(n);%消去for i = 1:n-1%消去for j = i+1:n%计算得到乘子:mLS = eye(n);m = M(j,i)/M(i,i);M(j,:) = M(j,:)-m*M(i,:);LS(j,:) = LS(j,:) - m*LS(i,:);L = L*LS;endendU = M(:,(1:n));%回代过程bn = M(:,n+1);X(n) = bn(n)/M(n,n);for k = 1:n-1X(n-k) = (bn(n-k)-M(n-k,(n-k+1:n))*X(n-k+1:n))/(n-k);endend 列主元高斯消元法之前的高斯消元法不是对主元要求很高吗, 既不能为 $0$ 也不能很小我们就来挑一挑主元, 把大的数作为主元不就好了吗我们知道, 把方程组的两行换一下, 方程组本质上是没有变的 (解也没有变) [!note] 列主元选取在顺序高斯消元法的基础上作出改进:再第 $k$ 步消元的时候, 我们在要消去的第 $k$ 个主元所在的行以及下面的行中选择选出第 $k$ 个位置最大的行, 和我们现在这一行交换(1) 先选取列主元：$|a_{i_kk}^{(k)}|=\\max_{k\\leq i\\leq n}\\left{|a_{ik}^{(k)}|\\right}\\neq\\mathbf{0}$(2) 如果 $i_k\\neq k$ 则交换第 $k$ 行和第 $i_k$ 行(3) 消元 本质上, 我们可以这样看我们先引入一个置换矩阵 $P$, 它的作用是对矩阵 $A$ 的行重新排一下序 (相当于提前换好行)只要先用这个 $P$ 去乘矩阵 $A$, 接下来就做一般的顺序高斯消元法就好了我们知道左乘相当于初等行变换$$PA = LU$$ 具体操作例如$$\\begin{pmatrix}10^{-8}&amp;2&amp;3\\-1&amp;3.712&amp;4.623\\-2&amp;1.072&amp;5.643\\end{pmatrix}\\begin{pmatrix}x_1\\x_2\\x_3\\end{pmatrix}=\\begin{pmatrix}1\\2\\3\\end{pmatrix}$$此时我们处于 $k=1$ 这一步, 于是从这行开始往下找, 可以发现第三行是最大的, 那就交换一三行 由于在消元的时候, 我们有$$a_{ij}=a_{ij}-\\frac{a_{ik}}{a_{kk}} \\times a_{kj}$$但是我们这个 $a_{kk}$ 已经比下面的行($i&gt;k$)里的都要大了(本列最大), 可以保证 $\\frac{a_{ik}}{a_{kk}}&lt;1$,这意味着如果 $a_{kj}$ 有误差, 这个误差一定不会扩大, 是越来越小的 (乘以一个小于 1 的数) 这样就保证了列主元高斯消元法的稳定性更好 ![[Pasted image 20240414000731.png|238]] 优缺点优点: 主元有 $0$ 的时候也可以继续 当主元比较小的时候也不会放大误差 缺点: 要求 $\\left| A \\right| \\ne 0$ 代码12345678910111213141516171819202122232425262728293031function [ X, U ] = colGaussElim( A, b )M = [A b];%增广矩阵[n, ~] = size(A);X = zeros(n,1);%消去for i = 1:n-1%换主元N = abs(M);[~, e] = max(N((i:n),i));%找出该列中主元绝对值最大的元素所在行的索引e=e+i-1;temp = M(i,:);M(i,:) = M(e,:);M(e,:) = temp;%替换结束if M(i,i) == 0error('奇异阵');end%消去for j = i+1:n%计算得到乘子:mm = M(j,i)/M(i,i);M(j,:) = M(j,:)-m*M(i,:);endendU = M(:,(1:n));%回代过程bn = M(:,n+1);X(n) = bn(n)/M(n,n);for k = 1:n-1X(n-k) = (bn(n-k)-M(n-k,(n-k+1:n))*X(n-k+1:n))/(n-k);endend PLU 分解和列主元高斯消元法的想法一样, 为了避免主元很小或者主元为 $0$ 时消元无法进行我们对矩阵 $A$ 的行进行交换, 把大的数作为主元这就需要我们引入一个置换矩阵 $P$, 提前对 $A$ 进行排序, 保证消元的精度 推导过程和 $LU$ 分解是类似的, 但是增加了一个记录行变换的步骤: 推导[[PLU分解推导]] PLU 分解回代过程一旦知道 $L$ 和 $U$ 还有 $P$, 问题 $Ax = b$ 就可以写成 $LUx=Pb$, 我们定义辅助向量 $c=Ux$那么回代过程就变成了两步: 对于方程 $Lc=Pb$, 求解 $c.$ 对于方程 $Ux=c:$, 求解 $x.$由于 $L$ 和 $U$ 都是三角形的矩阵, 两步运算都很直接 注意这里左边的 $P$ 是隐含在 $LU$ 中的 完全主元法顾名思义, 就是在整个矩阵右下角找主元 (因为左上角已经变成对角的了)如果是第一步, 那就是在整个矩阵里找主元 相比列主元法, 其实就是在做列变换的同时, 也做行变换 在剩余矩阵中找最大的元素 [!note] 完全主元在列 高斯消元法的基础上作出改进:再第 $k$ 步消元的时候, 我们在包括要消去的第 $k$ 个主元的右下角的矩阵中选择选出最大的元素, 通过行列变换(1) 先选取主元：$|a_{i_kj_k}^{(k)}|=\\max_{k\\leq i\\leq n,k\\le j \\le n}\\left{|a_{ij}^{(k)}|\\right}\\neq\\mathbf{0}$(2) 如果 $i_k\\neq k$ 或 $j_{k}\\ne k$ 把 $a_{i_kj_k}$ 交换到 $a_{kk}$ 的位置(3) 消元 本质上, 我们可以这样看我们先引入两个置换矩阵 $P$ 和 $Q$, $P$ 的作用是对矩阵 $A$ 的行重新排一下序 (相当于提前换好行), $Q$ 的作用是对矩阵 $A$ 的列重新排一下序 (相当于提前换好列)只要先用这个 $P$ 去左乘矩阵 $A$, 用这个 $Q$ 去右乘矩阵 $A$我们知道左乘相当于初等行变换, 右乘相当于初等列变换$$PAQ = LU$$ 假如这个矩阵里的所有元素中, 最大的三个元素为 $\\lambda_1&gt;\\lambda_2&gt;\\lambda_3$![[Pasted image 20240414004402.png]]完全主元法实际上就是做了这样的事 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function [P, Q, L, U] = TotalGaussElim(A)[m, n] = size(A);% 初始化PQP = eye(m);Q = eye(n);% 初始化LUL = zeros(m, m);U = zeros(m, n);for k = 1:min(m-1, n)% 找到主元位置[~, pivot_row] = max(abs(A(k:m, k:n)));[~, pivot_col_idx] = max(pivot_row);pivot_row = pivot_row(pivot_col_idx) + k - 1;pivot_col = pivot_col_idx + k - 1;% 行列变换if pivot_row ~= k% 对A行变换temp = A(k, :);A(k, :) = A(pivot_row, :);A(pivot_row, :) = temp;% 对P行变换temp = P(k, :);P(k, :) = P(pivot_row, :);P(pivot_row, :) = temp;% 对L行变换temp = L(k, 1:k-1);L(k, 1:k-1) = L(pivot_row, 1:k-1);L(pivot_row, 1:k-1) = temp;endif pivot_col ~= k% 对A列变换temp = A(:, k);A(:, k) = A(:, pivot_col);A(:, pivot_col) = temp;% 对Q列变换temp = Q(:, k);Q(:, k) = Q(:, pivot_col);Q(:, pivot_col) = temp;% 对U列变换temp = U(:, k);U(:, k) = U(:, pivot_col);U(:, pivot_col) = temp;end% 计算LUL(k:m, k) = A(k:m, k) / A(k, k);U(k, k:n) = A(k, k:n);% 高斯消元for i = k+1:mA(i, k:n) = A(i, k:n) - L(i, k) * U(k, k:n);endend% L的最后元素为1L(end, end) = 1;end PQLU 分解这个分解和完全主元法完全对应$$PAQ = LU$$为了目录结构的完整性, 所以专门列为一个标题, 但是内容是和完全主元法一致的相信看完前面 [[#推导|PLU分解]] 的推导, 这个也是很容易推出来的 追赶法 (Crout) 也称为Crout分解对于系数矩阵三对角矩阵的线性方程组, 它的结构比一般的线性方程组简单很多我们理应有更好的解法 [!note] 三对角矩阵形如以下矩阵为三对角矩阵:$$\\begin{bmatrix}\\mathrm{a}1&amp;\\mathrm{c}1\\\\mathrm{d}2&amp;\\mathrm{a}2&amp;\\mathrm{c}2\\&amp;\\ddots&amp;\\ddots&amp;\\ddots\\&amp;&amp;\\mathrm{d}{\\mathrm{n}-1}&amp;\\mathrm{a}{\\mathrm{n}-1}&amp;\\mathrm{c}{\\mathrm{n}-1}\\&amp;&amp;&amp;\\mathrm{d}{\\mathrm{n}}&amp;\\mathrm{a}{\\mathrm{n}}\\end{bmatrix}\\begin{bmatrix}\\mathrm{x}1\\\\mathrm{x}2\\\\vdots\\\\mathrm{x}{\\mathrm{n}-1}\\\\mathrm{x}{\\mathrm{n}}\\end{bmatrix}=\\begin{bmatrix}\\mathrm{b}1\\\\mathrm{b}2\\\\vdots\\\\mathrm{b}{\\mathrm{n}-1}\\\\mathrm{b}{\\mathrm{n}}\\end{bmatrix}$$ 对于三对角矩阵, 如果它满足[[#追赶法的条件]], 就一定有如下分解:$$A=LU$$其中$$L=\\begin{bmatrix}l_{11}&amp;0&amp;…&amp;0\\l_{21}&amp;l_{22}&amp;…&amp;0\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\l_{n1}&amp;l_{n2}&amp;…&amp;l_{nn}\\end{bmatrix},U=\\begin{bmatrix}1&amp;u_{21}&amp;…&amp;u_{n1}\\0&amp;1&amp;…&amp;u_{n2}\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\0&amp;0&amp;…&amp;1\\end{bmatrix}$$ 追赶法的条件 [!note] 追赶法条件若 $A$ 为对角占优三对角阵，且满足$|b_1|&gt;|c_1|&gt;0,|b_n|&gt;|a_n|&gt;0,\\quad a_i\\neq0,c_i\\neq0$ $|b_{i}:|\\ge|:a_{i}:|+|:c_{i}:|:,:a_{i}\\cdot c_{i}\\neq0\\quad i=2,\\cdots,n-1$ 则方程组有唯一的 LU 分解 对应的矩阵形式: $$A=LDM=TM$$其中 $T$ 是单位下三角矩阵, $M$ 是上三角矩阵更彻底的分法其实是 $LDM$, 其中 $L$ 是单位下三角矩阵, $D$ 是对角矩阵, $M$ 是单位上三角矩阵 显然这个条件蕴含了三对角矩阵 A 的各阶顺序主子式都不为零 具体操作操作起来是很简单的, 我找到一张很棒的图:![[1ded1536454857.jpg]]非常直观 平方根法(Cholesky 分解)对于三对角矩阵我们有很方便的解法, 那么对于对称正定矩阵又如何呢?我们对于对称正定矩阵, 可以做 Cholesky 分解先了解一下对称正定矩阵的特点: [!note] 对称正定矩阵性质如果 $A$ 是一个对称正定矩阵, 那么: $A$ 的所有特征值 $&gt;0$ $A$ 是非奇异矩阵, 也就是各行各列线性无关, 行列式为 $\\ne0$ $A^{-1}$ 也是对称正定的 $A$ 的对角线元素 (主元) $a_{ii}&gt;0\\quad(i=1,2,\\cdots,n)$ $A$ 的所有顺序主子式均 $&gt;0$, 即 $\\det(A_{k})&gt;0$ 我们先来看看对称矩阵能分解成什么样 [!note] 对称矩阵的三角分解设 $A$ 是对称矩阵, 如果 $A$ 的所有顺序主子式都 $\\ne0$, 那么 $A$ 有唯一的分解:$$A=LDL^{T}$$其中 $L$ 为单位下三角矩阵, $D$ 为对角矩阵 直观地理解, 是很简单的, 其实就是消元:我们通过 $L^{-1}$ 进行初等行变换消掉下三角部分, 由于这个矩阵是对称的把 $L^{-1}$ 转置一下, 不就能消掉上三角部分了吗?其实是 $L^{-1}A{L^{-1}}^T=D$ 现在我们更进一步, 看看对称正定矩阵可以分解成什么样我们已经可以得到 $A=\\tilde{L}D\\tilde{L}^T$ (这里为了防止冲突, 我们把 $L$ 记为 $\\tilde{L}$)我们记 $D=diag(d_1,d_2\\cdots d_n)$ ($diag$ 指的是对角的意思), 由于 $A$ 是正定的由性质 1, 我们可以把 $D$ 拆成两半$$D^{\\frac12}=diag(\\sqrt{d_1},\\sqrt{d_2}\\cdots\\sqrt{d_n})$$容易验证 $D^{\\frac{1}{2}}\\times D^\\frac{1}{2}=D$又由于 $D^{\\frac{1}{2}}={D^{\\frac{1}{2}}}^T$, 我们可以得到:$$A=\\tilde{L}D^{\\frac{1}{2}}{D^{\\frac{1}{2}}}^T\\tilde{L}^T=\\tilde{L}D^{\\frac{1}{2}}({D^{\\frac{1}{2}}}\\tilde{L})^T$$ 如果我们记 $\\tilde{L}D^{\\frac{1}{2}}=L$, 就得到:$$A=LL^T$$这就是 $Cholesky$ 分解 [!note] 对称正定矩阵的 Cholesky分解若 $A$ 对称正定, 则 $A$ 可以唯一分解为$$A=LL^T$$其中 $L$ 为实下三角矩阵, 对角元素均 $&gt;0$ 具体操作![[Pasted image 20240414235714.png]]如图所示, 只要把右侧矩阵乘起来, 一一对应位置来求解 计算顺序：先求 $L$ 的第一列，即可知道 $L^T$ 的第一行，然后再求解 $L$ 的第二列，….，以此类推 计算公式如下:$$a_{ij}=\\sum_{k=1}^nl_{ik}l_{jk}=\\sum_{k=1}^{j-1}l_{ik}l_{jk}+l_{jj}l_{ij}$$给出一个例子:[[Cholesky分解例子]] Cholesky 分解回代过程和上面类似解方程：$Ly=b$ 和 $L^Tx=y$ 优缺点优点: 省内存:各值可以存放在原矩阵对应位置，不必开辟新的存储空间 不用选主元 具有数值稳定性 由于 $A$ 的对称性, 运算量为 $LU$ 分解的一半缺点: 需要进行 $n$ 次开方运算, 而开放运算一定是有误差的, 还需要额外计算量 改进的 Cholesky分解运用平方根法计算量较大，为了避免开方运算，可以使用改进的 $Cholesky$分解 [!note] 改进 Cholesky 分解$A$ 为对称正定矩阵, 则我们改进的 $Cholesky$ 分解为:$$A=LDL^T$$其中 $L$ 为单位下三角矩阵, $D$ 为对角矩阵 说是改进, 其实这个是我们推出 $Cholesky$ 的中间产物, 之前已经见过了 具体操作$$A=LDL^T=\\begin{bmatrix}1&amp;&amp;&amp;&amp;\\l_{21}&amp;1&amp;&amp;&amp;\\\\vdots&amp;&amp;\\ddots&amp;\\l_{n1}&amp;\\cdots&amp;l_{n,n-1}&amp;1\\end{bmatrix}\\begin{bmatrix}d_1&amp;&amp;&amp;\\&amp;d_2&amp;&amp;\\&amp;&amp;\\ddots&amp;\\&amp;&amp;&amp;d_n\\end{bmatrix}\\begin{bmatrix}1&amp;l_{21}&amp;\\cdots&amp;l_{n1}\\&amp;1&amp;&amp;l_{n2}\\&amp;&amp;\\ddots&amp;\\vdots\\&amp;&amp;&amp;1\\end{bmatrix}$$和上面的差不多, 我们把右边乘开, 对上位置列方程计算就行了有计算公式:$$a_{ij}=\\sum_{k=1}^nl_{ik}d_kl_{jk}=\\sum_{k=1}^{j-1}l_{ik}d_kl_{jk}+l_{ij}d_j$$ 改进 Cholesky 分解的回代过程和前面类似, 就是求解 $Ly=b$ 和 $DL^Tx=y$ 优缺点优点: 相比 Cholesky 分解不用进行开方运算 下面已经没有了哦","link":"/2024/04/17/%E7%9B%B4%E6%8E%A5%E6%B3%95/"},{"title":"自编码器与变分自编码器","text":"界外章节: 章节 9:自编码器与变分自编码器矩阵分解系列教程迎来了尾声 引入我们之前一直在做矩阵分解,矩阵分解的内核是, 把高维的信息整合, 通过一些正则化手段降维, 以此提取主要特征, 我们的 qr 分解提取了正交特征, 特征值 (奇异值) 分解提取了各维度主特征, 都是一个降维的过程, 这类分解统称为因子分解上一章的非负矩阵分解通过将矩阵分解为两个非负矩阵的乘积，能有效揭示数据的潜在特征和结构, 这里我们会有更有效的方法 现在让我们来看看比较新的降维(升维)手段 要讲变分自编码器, 我们首先要讲自编码器才行自编码器 ($Autoencoder$, $AE$ (Applied energy)) 是一种神经网络, 用于学习数据的表示方式(编码), 是一种无监督学习架构他的内核和矩阵分解是一样的, 只是失去了有效的模式化的表示方法 (没有公式可以表达了)就像大自然的很多东西, 我们没有办法用公式完全地解析尽管如此, 我们仍然是有办法知道编码后的内容大概代表了什么信息, 不过这里不作赘述, 可以自己看论文 论文: 1904.05742 1804.02812 1905.05879它通过一种更灵活的方式提取矩阵的潜在规律 (这是不规则的), 并且可以以不同的维度转换信息 (而对输入输出没有要求)PCA 是一种线性降维, 而这是一种非线性降维,可以捕捉复杂的非线性相关性因此, 它用于降维, 特征提取, 数据重构 首先, 我们会介绍自编码器的架构, 然后是实现方法, 各种相关算法, 以及有意思的变分自编码器最后, 我会放上我的 $colab$ 的代码, 方便大家运行 自编码器架构学习神经网络, 首先要清楚这个网络的架构, 然后再考虑具体的求解方法神经网络由编码器 ($Encoder$) 和解码器 ($Decoder$) 组成它的平凡的实现是: 把输入复制到输出例如，给定一个手写数字的图像，自编码器首先将图像编码为低维的潜在表示，然后将该潜在表示解码回图像自编码器学习压缩数据，同时最大程度地减少重构误差![[Pasted image 20240510120453.png]]很像 $Bert$, 你可以把 $Bert$ 的前 $6$ 层看作 $Encoder$, 后 $6$ 层看作 $Decoder$ (如果是 $12$ 层那个的话)尽管 $Bert$ 比这玩意晚了十几年才出来 () $Decoder$ 可以看作一个 $Generator$, 吃一个向量, 产生一个东西 编码器编码器把输入的数据转换为一个更紧凑, 更低维的表示, 其实就是数据的降维它把数据映射到一个隐含的特征空间, 是一个向量 (大多数情况下, 也可以不是)例如, 它可能会把矩阵 $\\begin{pmatrix}6.12 &amp; 183 \\ 777 &amp; 555\\end{pmatrix}$ 映射到 $\\begin{pmatrix}9999 &amp; 0 &amp; 132 &amp; 123\\end{pmatrix}$, 这当然是我随便想的, 但这也是有可能的,因为编码器内部数据就是一个向量, 里面的每一个具体的位置, 都隐含了很多意义(以及和其他元素的联系), 而不是传统意义上的位置信息这个例子想说明的是, 自编码器不是通过某些数学规律来提取特征的, 而是通过在大量矩阵数据输入的学习中, 学到了内部的潜在规律, 就像你无法说明你是由什么数学原理而学会了说话一样, 唯一的解释是: 大量的经验, 在经验的积累中, 在某一刻产生了质变 中间的向量 ($code$) 包含了很多信息, 如果是图片, 就包含了图片的主要信息, 亮度之类的如果是声音, 就包含了声音里几个主要的波什么的如果是文章, 就包含了文章的主要内容之类的对于声音,我们可以通过做 $Feature\\ Disentangle$, 来知道哪些维度代表了说话人的特征, 哪些部分代表了话语的内容, 其他也是类似的 编码过程可以表示为:$$Embedding = h(x)$$ 解码器解码器把内部特征空间混乱不堪的数据重新解释, 它学习如何重构输入, 并且尽可能准确地复原数据, 可以输出和输入相同的格式的内容, 也可以输出不同格式的内容就像人的记忆一样, 它解码的过程就是在对记忆进行回忆 解码过程可以表示为:$$r = f(Embedding)=f(h(x))$$ 其中, 两个函数 $f$ 和 $h$ 其实都是一个 $nn$ (神经网络), 使用矩阵运算:$$y = Wx + b$$其中 $W$ 为权矩阵, $b$ 称为 $bias$ (偏置) 网络训练自编码器网络的训练机理为: 最小化重构误差这个过程使得自编码器能够捕捉并学习输入数据的重要特征，同时去除不必要的噪声或冗余信息如果输入矩阵 $A$, 输出矩阵 $B$, 那么训练过程就是一个优化模型, 优化目标为:$$\\underset{网络参数}{\\min} (\\left|| A-B \\right||_{loss})$$当然这个误差表示有很多种,一般是均方误差($MSE$), 也可以用平方和, 也可以用别的 我们通过对比输入和输出 , 求出预测误差，进行反向传递, 更新网络参数，逐步提升自编码的准确性 求解这个优化模型一般用梯度下降家族的方法 ($Adam$ 或者 $RMSprop$)为了避免过拟合, 应该使用 $Early\\ Stopping$ 或者正则化 ($L1$ 或 $L2$) 堆栈自编码器有些时候我们需要多层自编码器 (如后面的分类任务), 这个时候我们的网络就应该深一点如图:![[Pasted image 20240510120708.png]]这张图片是有三个隐藏层的堆栈自编码器, 层数多了就变成深度自编码器注意, 图片里的方块大小是有意义的, 这意味着我们的两个编码过程在不断降低特征的维度而我们的解码过程在不断提升特征的维度 应用预训练很久之前由于深度网络是随机初始化的, 所以训练很慢效果也不好, 就没有引起重视直到开始使用一些手段来初始化比较好的各个网络层级假如有一个这样的神经网络, 它的网络架构如下:![[Pasted image 20240508221628.png]]我们怎么预训练第一层的网络呢? 使用一个自编码器:![[Pasted image 20240508221958.png|368]]把这个训练到收敛, 我们就取这个自编码器的 $Encoder$ 作为初始参数 压缩之前说到我们通过 $Encoder$ 得到的东西是一个低维的向量, 这其实就可以看作一种压缩, 而 $Decoder$ 做的事情就是解压缩但是这个压缩是 $Lucy$ 的, 也就是说会失真 VITSVITS 就是这个架构的, 不知道是啥? 查一下就知道了过去我们要模仿别人的声音, 需要做监督学习, 就是需要有标注的数据比如我想模仿 A 的声音, 我需要的数据有什么呢?我说一句”早上好”, 必须让 A 也来说一句”早上好”并且这样的成对数据需要很多, 但这是不现实的, 如果 A 不会中文呢? 这就做不到了我们这个时候就可以用自编码器, 如果我们做了 $Feature\\ Disentangle$, 就能把 $code$ 分成下面那样:![[Pasted image 20240508195937.png]] 分类器之前我们说到 $Embedding$ 或者是 $code$ 是一个向量, 如果换成别的呢?如果我们换成一个 $Binary$, $Binary$ 是一个逻辑数组, 里面的内容都是 $bool$ 型的, 只能取 $0$ 或 $1$类似于决策树, 我们可以根据很多特征来分类:![[Pasted image 20240508201009.png]]这样就根据一个特征分类了, 把很多个自编码器类似于决策树一样堆起来, 就能做分类了 如果我们换成一个 $One-hot$, $One-hot$ 是一个逻辑数组, 它只有一个位为 $1$, 其他位都为 $0$![[Pasted image 20240508201130.png]]通过这个我们可以实现 $UNsupervise$ 的分类, 也就是无标签分类 在分类时, 损失函数应该使用交叉熵损失 VQVAE$Vector\\ Quantized \\ Variational\\ Auto-encoder$ –基于离散隐变量的生成模型这个才是最出名的其实就是在我们的网络架构中添加了一个 $code\\ book$ 具体操作如图:![[Pasted image 20240508202500.png]]在把一个图片用 $Encoder$ 处理之后, 我们不直接把结果送到 $Decoder$, 再计算误差而是从准备的 $code\\ book$ 中找到相似度最高的向量, 然后把那个向量送到 $Decoder$, 再计算误差通过最小化误差, 我们会更新我们的 $Encoder$, $Decoder$, 还有 $code\\ book$ 这样的好处是, 我们的 $code\\ book$ 有机会学到本质的东西如果是图片, 训练后我们的 $code\\ book$ 里可能是图片的类型, 风景图人物图啊之类的如果是声音, 我们的 $code\\ book$ 里可能是最基础的发音之类的 $Attention\\ Is\\ All\\ You\\ Need$ 文字我们甚至可以用一串文字代替 $Embedding$, 这段文字, 很有可能就是这篇文章的摘要但是这里的 $Encoder$ 和 $Decoder$ 就需要是 $Sequence-To-Sequence$, 因为我们输入文章, 输出文字, 都是一串一串的这样的 $Auto-encoder$, 就是 $Sequence-To-Sequence-To-Sequence$ 的![[Pasted image 20240508203353.png]]这样做的好处是, 我们不需要标注就能让机器自己学会做摘要 你信了吗? 然而实践告诉我们这样行不通在训练的过程中, $Encoder$ 和 $Decoder$ 会发明自己的暗号!在提取摘要后, 里面会出现一段 (也许是分散的) 暗号, 这段暗号是你看不懂的, 但是 $Decoder$ 可以通过这段乱七八糟的暗号知道这段文章是啥, 它们就这样完成了训练! 而结果是这个模型会提取出一段乱七八糟的暗号而不是摘要 我们需要一个额外的模型称为 $Discriminator$, 它用人写的文本训练, 作用是判断一段文字能不能构成语言我们需要使 $Encoder$ 不仅可以编码成一个东西后能扔给 $Decoder$ 还原, 还必须把这个编码后的东西扔给 $Discriminator$ 校验, 必须让 $Discriminatot$ 认为这构成语言, 我们希望这样能强迫 $Encoder$ 可以提取大纲 懂一点点机器学习的朋友可能会说, 这中间产生文本还要扔给 $Discriminator$ 这做起来好麻烦啊, 这网络怎么搭呢? 我的评价是, 没办法 $train$ 的问题就用 $rl$ 硬做就行了 在我看来, 这根本就是 $Cycle\\ GAN$ TREE还有一个拿 $TREE$ 当 $Embedding$ 的案例, 这太亏贼了, 架构颠佬恐怖如斯就是一段文字变成 $TREE$, 再拿这个 $TREE$ 还原文字 论文: 1904.03746 变分自编码器 VAE原文: [1312.6114] Auto-Encoding Variational Bayes (arxiv.org)除了 $GAN$ 以外, 还有 $VAE$ 也是生成模型$VAE-Variational\\ Auto-encoder$, 看名字就知道它和 $Auto-encoder$ 很有关系 它其实就是把 $Auto-encoder$ 的 $Decoder$ 拿出来当成 $Generator$, 但是也做了其他事情下面细说 引入我们知道, 我们的 $Decoder$ 可以看作一个生成模型, 根据一个向量, 生成一个东西考虑以下情形:![[Pasted image 20240509225929.png|290]]我们一系列图片, 是月亮的不同形态, 现在我们假设我们把月亮的图片编码后的 $Embedding$ 是一个一维的向量 (也就是一个数, 一条直线)好, 现在我们把月圆的地方的 $code$ 放进 $Decoder$, 就会返回一张月圆的照片同样地, 把半月的地方的 $code$ 放进 $Decoder$, 就会返回一张半月的照片那么问题来了, 如果我们把这两个点中间的点放进 $Decoder$ 会怎么样呢?我们当然是期望返回一张介于月圆和半月之间的月亮的图, 但是事实上在绝大部分情况下, 这样生成的都是一些没有意义的照片, 运气好的话, 我们可能可以获得一些有意义的照片 为什么会这样呢? 这是因为我们没有对隐变量 (也就是 $Embedding$ 或者 $code$) $z$ 的分布进行估计直线空间 $\\mathbb{R}$ 已经很大了, 我们不知道有意义 (可以通过 $Decoder$ 转化为有意义的图) 的 $z$ 到底在这段直线上的哪个地方, 分布在哪个区间由于取值有无穷多个, 而有意义的只有有限多个, 我们还不知道有意义的分布在哪, 这样找有意义的 $z$ 就像大海捞针, 所以我们需要对隐变量 $z$ 进行建模, 要知道在哪些区间下可以生成哪些(类别的)图片 也就是说, 我们的模型应该能够适应一定范围的 $noise$, 即就算我们在这两个点中间取样, 模型的结果也不会偏离太多 (至少也应该是一张月亮)![[Pasted image 20240509230833.png|267]] 这就引出了变分自编码器的设计 架构变分自编码器把给每一个样本使用 $Encoder$ 学习一个分布, 再在学习的分布上随机采样, 将采样结果送入 $Decoder$, $Decoder$ 会尽量还原样本原始的分布基本的架构如下:![[Pasted image 20240509225008.png]]但是通常我们不需要求到样本的分布, 而是生成一个新的样本就够了, 于是网络变成这样:![[Pasted image 20240509235932.png]]这就是我们主要考虑的架构这个结构就和自编码器差不多了, 只是中间多了一步根据每个样本拟合正态分布, 再随机取样的过程 向 $Encoder$ 输入一个数据点 $x_{i}$, 通过 $Encoder$ ($nn$) 我们得到 $x_{i}$ 对应的隐变量 $z$ 服从的后验分布 $p(z|x_{i})$ 的参数 $u_{i}$ 和 $\\sigma_{i}^{2}$ 根据 $u_{i}$ 和 $\\sigma_{i}^{2}$ 我们可以生成对应的正态分布, 从这个正态分布中采样得到一个 $z_{i}$, 这个 $z_{i}$ 就应该是和 $x_{i}$ 强相关的 向 $Decoder$ 输入一个隐变量点 $z_{i}$, 通过 $Decoder$ ($nn$) 我们得到 $z_{i}$ 对应的生成样本点 $X$ 服从的条件分布 $p(X|z_{i})$ 的参数 我们可以从这个分布 $p(X|z_{i})$ 中采样, 也可以直接拿均值 $u’_{i}$ 作为生成的数据 符号参照表: $$\\begin{aligned}&amp;X:样本随机变量\\&amp;x_i:样本随机变量的一个取值\\&amp;z:隐变量随机变量\\&amp;z_i:隐变量随机变量的一个取值\\&amp;p(z):随机变量z符合的先验分布\\&amp;p(X|z_{i}):已知z_{i}的情况下,X的条件分布\\&amp;p(z|x_{i}):已知样本x_{i}下,z的后验分布\\&amp;p(x):样本分布的近似\\&amp;q:用来区分,其实q=p(z|x_{i})\\end{aligned}$$ 推导假设: z 的分布 思想: 由于 $z$ 的分布是很不确定的, 我们只能在 $\\mathbb{R}^n$ 上随机取样碰运气, 为什么我们不先假设 $z$ 符合一个简单的, 取值很有限的分布呢? 这样就能把采样的空间压缩的足够小 正态分布恰恰有这样的性质, 它在两侧的取样概率是很小的, 也就意味着值大多在中间 于是我们要求$z$ 符合分布 $z\\sim\\mathcal{N}(0,I)$, 其中 $I$ 为单位阵, 意味着 $z$ 符合多元正态分布, $z$ 是一个随机变量:![[Pasted image 20240509231441.png]](一定是正态分布吗? 其实不然, 只是正态分布既方便计算, 又便于推导, 它可以是任何东西)好的, 我们现在引入了随机变量的概念, 把 $z$ 转化为了随机变量, 由于 $z$ 是由样本 $X$ 编码的, 我们要求样本 $X$ 也为随机变量下面我们引入一些记号: $$\\begin{aligned}&amp;X:样本随机变量\\&amp;x_i:样本随机变量的一个取值\\&amp;z:隐变量随机变量\\&amp;z_i:隐变量随机变量的一个取值\\end{aligned}$$ 还有一些记号: $$\\begin{aligned}&amp;p(z):随机变量z符合的先验分布\\&amp;p(X|z_{i}):已知z_{i}的情况下,X的条件分布\\end{aligned}$$ 那么我们之前架构中提到的过程就变为: 在先验分布 $p(z)$ 中随机采样得到一个 $z_i$ 根据 $z_i$, 从条件分布 $p(X|z_i)$ 中随机采样得到一个数据点 $x_{i}$, 这个数据点就是我们 $Decoder$ 生成的结果, 在大部分情况下, 我们采样的地方为均值 $\\mu$, 我们的 $\\sigma$ 作为一个超参数被抛弃这样就能做到一件事–在已知两个分布之后, 通过采样来生成数据 目标我们的生成的最终目标是什么?我们就是想在一个很接近真实分布 $P_{real}(X)$ 的分布 $p(x)$ 中采样, 生成一些有意义的数据我们引入记号:$$p(x):样本分布的近似$$![[Pasted image 20240509232538.jpg]]比如这个图里有很多宝可梦 (看到这里应该已经有人知道是哪位老师的课了), 我们希望在 $P(x)$ 值比较大的地方取点, 得到一些 $P(x_i)$ 值比较大的样本点 $x_i$, 然后这些样本点就很有可能是宝可梦 (也许是杂交的) 假设: X 的分布那么我们怎么用数学表示这个奇怪的分布 $P(x)$ 呢? 使用高斯混合模型 我们假定只要给定某个 $z_i$, $X$ 都服从各维度独立的多元高斯分布, 即$$p(X\\mid z_i)=\\mathcal{N}(X\\mid\\mu_i^{\\prime}(z_i;\\theta),\\sigma_i^{\\prime2}(z_i;\\theta)*I)$$这表示取定一个 $z_{i}$, 它就是高斯混合模型中的一个子正态分布 高斯混合模型就能很好地近似这种多峰的情况, 而且由于它是由很多正态分布叠加形成的, 我们可以很简单地处理 (不一定效果最好, 但是能行)![[Pasted image 20240509233117.png]]可以看到, 里面有很多子正态分布, 高斯混合模型就是由很多子正态分布叠加形成的 我们之前提到, $p(X|z_{i}):已知z_{i}的情况下,X的条件分布$, 由于 $z_i$ 的取值是连续平滑的, 我们理所当然有:$$X的分布=\\int z的分布\\times 在某个z下X的条件分布$$即:$$p(x)=\\int_{z}p(X|z)p(z)dz$$类似一个全概公式表现为:![[Pasted image 20240509234016.png|475]]即我们取一个 $z$ 就确定了一个 $p(x)$ 中的子正态分布 现在我们先不讲具体的每个分布怎么求, 就先假定已经求到了, 那么现在是不是只要我们采样很多个 $z_i$, 就能通过积分求得 $p(x)$ 了?可以是可以, 但是代价非常大, 因为 $x_i$ 的维度往往很大, $z_i$ 的维度也很大, 这就导致对于某个 $x_i$, 与之强相关的 $z_i$ 很少很少, 我们很难取样到, 很有可能我们取样到的 $z_i$ 和哪个样本都没关系 于是从正态分布 $p(x)$ 中直接采样得到 $z_i$, 再用来估计 $p(x)$, 是几乎不可行的下面我们来解决这个问题 修正 z 的分布, 提高采样效率我们为了解决很难采样到和 $x_i$ 强关联的 $z_i$ 这个问题, 引入新的符号:$$p(z|x_{i}):已知样本x_{i}下,z的后验分布$$直觉地, 在这个分布下采样得到的 $z$ 应当都和 $x_i$ 有很强的关系 由于我们先前的推导都是假定 $z$ 为正态分布的基础上进行的, 我们理应要求这个后验分布 $p(z|x_{i})$ 也应该符合正态分布 (修正的正态分布) 用图来表示的话, 就是这样的策略:![[Pasted image 20240509235244.jpg|400]]这样我们采样的 $z$ 都和 $x_i$ 有很强的关系, 就省去了很多采样步骤, 提高了效率 计算各个分布我们之前一共提到四个分布, 我们分别来考虑怎么求: $$\\begin{aligned}&amp;p (z): 随机变量 z 符合的先验分布\\&amp;p (X|z_{i}): 已知 z_{i}的情况下, X 的条件分布\\&amp;p(z|x_{i}):已知样本x_{i}下,z的后验分布\\&amp;p(x):样本分布的近似\\end{aligned}$$ $p(z)$由于这个 $z$ 的先验分布我们已经用 $p(z|x_{i})$ 取而代之了 (为了更好地取样), 所以不用求了 $p(X|z_{i})$我们采样一个 $z_{i}$, 要求 $X$ 的分布, 其实就是给定一个向量, 输出均值和方差的问题这种问题我们交给 $nn$ (神经网络) 去做, 没有思路, 难算, 就用神经网络拟合出来, 就是这么简单粗暴, 因为我们知道这个分布是一个正态分布, 所以只要拟合参数就行了 这个算法叫做变分贝叶斯算法 (变分就变分在这)这里其实就是 $Decoder$ 的部分, $Decoder$ 要做的就是根据一个 $z_{i}$ 拟合出分布在 $WGAN$ 中, 我们也是这样解决问题的 $p(z|x_{i})$我们采样一个 $x_{i}$, 要求 $z$ 的分布, 其实也是给定一个向量, 输出均值和方差的问题我们一样交给 $nn$ 这个算法叫做变分贝叶斯算法 (变分就变分在这)这里其实就是 $Encoder$ 的部分, $Encoder$ 要做的就是根据一个 $x_i$ 拟合出分布 $p(x)$这是我们对样本真实的分布使用高斯混合模型做的一个近似, 我们有一个公式求它:$$p(x)=\\int_{z}p(X|z)p(z)dz$$只要我们求到了 $p(X|z)$ 和 $p(z)$, 我们就能通过积分算出来这个分布 重参数技巧原文就是”$Reparameterization\\ Trick$”, 我们来看看这个是用来干啥的 我们的架构中, 从神经网络训练的角度来看, 前向传播是可以做的, 因为后面每一步需要接受的参数我们都能算出来, 然而, 我们在前向传播的过程中, 使用了一个”从后验分布 $p(z|x_{i})$”中随机采样的操作, 这个随机采样的操作怎么反向传播?肯定不能, 直接从这个分布采样会导致无法通过梯度下降法来优化参数，因为采样操作本身是不可微的我们得加一个参数, 来实现”可以反向解释的”随机采样 得到分布 $p(z|x_{i})$ 之后, 我们先从多元正态分布 $N(0,I)$ 中采样得到一个 $\\epsilon_{i}$, 进行如下计算:$$z_{i}=\\mu_{i}+\\sigma_{i}\\cdot \\epsilon_{i}$$其中 $\\epsilon$ 是可微的, 它的分布是正态分布这样反向传播就可以跑通了 变分贝叶斯推断我们之前说, 如果我们知道 $p(z|x_{i})$ 这个后验分布是正态分布, 我们用神经网络去拟合出这个 $u_{i}$ 和 $\\sigma_{i}$, 这个过程就是变分贝叶斯推断我们来看看这个过程具体怎么做:要计算后验概率, 我们有贝叶斯公式, 注意, 分母等价于一个连续的全概公式$$p(z|x)=\\frac{p(z)p(x|z)}{\\int p(z)p(x|z)dz}$$分子是好算的, 因为我们的先验概率 $p(z)$ 假设为了标准正态分布, 似然分布假设为了正态分布, 其中两个参数就是我们的模型需要学出来的 对于分母现在有两种思路： 用一个相对简单的分布 $q(z|x)$ 去近似 $p(z|x)$ ,这是变分推断要做的事 尝试对$p(z|x)$进行取样，得到样本后，自然可以计算期望，这是 $MCMC$ 的思路 我们这里使用变分推断:这里我们需要知道 $KL$ 散度 (后面会介绍),如果 $KL$ 散度越小, 说明我们的变分分布越接近真实的后验分布于是, 我们只要转成优化问题, 优化模型参数使得 $KL$ 散度最小就行了这就是变分推断的过程 损失函数我们在讨论损失函数之前, 需要一些预备知识: 信息论我们要用到信息论中的信息熵, 交叉熵, 相对熵 (KL 散度)信息论是很有意思的, 这里提一下: 信息等于编码 信息熵我们的信息熵可以看作一个事件包含的信息量, 也可以看作惊奇程度确定性越大, 熵越大, 定义为:$$H(x)=-\\sum\\limits_{x}\\log p(x)$$其中 $p(x)$ 是事件 $x$ 发生的概率, 我们可以发现, 概率越小的事件信息熵越大 信息论中, 表示该信息需要的编码长度 如果 $X$ 是一个离散型随机变量, 那就要求平均编码长度了, 表示为:$$H(p)=-\\sum\\limits_{i=1}^{n}p(x_{i})\\log p(x_{i})$$其实就是一个加权平均 交叉熵现在我们考虑一个分布 $q$, 我们希望用分布 $q$ 去逼近分布 $p$我们定义交叉熵为:$$H(p,q)=-\\sum\\limits_{i=1}^{n}p(x_{i})\\log q(x_i)$$这意味着我们通过错误的分布 $q$ 进行编码, 算出来的期望编码长度当我们的拟合效果很好时, $q\\to p$, 这个时候交叉熵就逼近 $p$ 的熵, 也就是逼近 $p$ 的最优编码长度 也就是交叉熵越小, 两个分布越接近 相对熵 (KL 散度)$KL\\ Divergence$为了区分两个分布, 我们记 $q=p(z|x_i)$ 为 $Encoder$ 计算出的后验分布定义如下:$$D_{KL}(p||q)=-\\sum\\limits_{i=1}^{n}p(x_{i})\\cdot \\log \\frac{q(x_{i})}{p(x_{i})}$$这样可能不是很好看, 把它拆开:$$\\begin{aligned}D_{KL}(p|q)&amp;=-\\sum_{i=1}^np(x_i)\\cdot\\log\\frac{q(x_i)}{p(x_i)}\\&amp;=-\\sum_{i=1}^np(x_i)\\cdot\\log q(x_i)+\\sum_{i=1}^np(x_i)\\cdot\\log p(x_i)\\&amp;=H(p,q)-H(p).\\end{aligned}$$意味着利用拟合的分布 $q$ 去编码 $X$ 得到的期望编码长度减去 $X$ 的最佳编码长度也就是利用 $q$ 编码 $X$ 所带来的额外编码长度 (可以理解为误差的一个量化) 这意味着, 优化 KL 散度, 就是优化两个分布之间的误差当 $KL散度\\to 0$, 我们的误差也 $\\to 0$ 这意味着我们可以使用 KL 散度作为损失函数的一部分 极大似然估计我们希望在分布 $p(X|z_{i})$ (也就是最终的分布的一个子正态分布) 中概率大的地方对应了我们的 $x_{i}$, 这就是极大似然估计的思想就是我们希望我们模拟的这个混合高斯模型能最接近真实的分布情况即我们要最大化 $$\\log p(X) = \\sum\\limits_{x}\\log p(x)$$ 损失函数框架看完本文再来品味这个框架是最佳食用方式首先根据我们的目标, 是希望通过混合高斯模型拟合出 $z$ 生成 $x$ 的分布, 自然需要用到 $MLE$ (极大似然估计), 我们需要让这个拟合分布最接近真实的分布, 这就是我们想法的出发点 然后, 我们根据这个框架, 通过公式的变换,可以拆分成两个子问题 我们希望 $Encoder$ 拟合的后验分布能最接近真实的分布 我们希望最大化 $Decoder$ 的拟合的混合高斯分布对真实分布的似然度 真正的损失函数由于我们采样的 $x$ 实际上可以有无穷多个值, 就变成积分, 再变换一下:$$\\begin{aligned}\\log p(X)&amp;=\\int_zq(z\\mid X)\\log p(X)dz\\quad\\text{全概公式}\\&amp;=\\int_zq(z\\mid X)\\log\\frac{p(X,z)}{p(z\\mid X)}dz\\quad\\text{贝叶斯定理}\\&amp;=\\int_zq(z\\mid X)\\log\\biggl(\\frac{p(X,z)}{q(z\\mid X)}\\cdot\\frac{q(z\\mid X)}{p(z\\mid X)}\\biggr)dz\\&amp;=\\int_zq(z\\mid X)\\log\\frac{p(X,z)}{q(z\\mid X)}dz+\\int_zq(z\\mid X)\\log\\frac{q(z\\mid X)}{p(z\\mid X)}dz\\&amp;=\\ell\\left(p,q\\right)+D_{KL}\\left(q,p\\right)\\&amp;\\geq\\ell\\left(p,q\\right)\\quad KL\\text{散度非负}\\end{aligned}$$再调整一下顺序:$$\\ell\\left(p,q\\right)=\\log p(X)-D_{KL}\\left(q,p\\right)$$这意味着只要我们最大化 $\\ell$ 就能最大化似然度 $\\log p(X)$ 并且最小化 $KL$ 散度 $D_{KL}\\left(q,p\\right)$这意味着我们的 $Encoder$ 拟合的后验分布 $p(z|x_{i})=q$ 会最接近我们真实的后验分布 $p$不然 $Encoder$ 可能会输出一些没什么意义的分布 为了方便符号表示, 我们引入记号来区分先验分布和后验分布:这也是变分贝叶斯推断的思想, 引入一个变分分布 $q_{\\phi}(z|X)$ 来近似真实分布 $q_\\theta(z|X)$$$q_\\phi(z\\mid X):近似的后验分布$$$$q_\\theta(z\\mid X):真实的后验分布$$ $$\\begin{aligned}\\ell\\left(p_{\\theta},q_{\\phi}\\right)&amp;=\\int_zq_\\phi(z\\mid X)\\log\\frac{p_\\theta(X,z)}{q_\\phi(z\\mid X)}dz\\&amp;=\\int_zq_\\phi(z\\mid X)\\log\\frac{p_\\theta(X\\mid z)p(z)}{q_\\phi(z\\mid X)}dz\\quad\\text{贝叶斯定理}\\&amp;=\\int_zq_\\phi(z\\mid X)\\log\\frac{p(z)}{q_\\phi(z\\mid X)}dz+\\int_zq_\\phi(z\\mid X)\\log p_\\theta(X\\mid z)dz\\&amp;=-D_{KL}\\left(q_\\phi,p\\right)+\\mathbb{E}_{q_\\phi}\\left[\\log p_\\theta(X\\mid z)\\right]\\end{aligned}$$ 左边的 KL 散度我们一般称为 $Latent\\ Loss$, 相当于一个正则项, 提高模型泛化能力的我们之前已经假设了 $q_\\phi(z\\mid X)$ 和 $p(z)$ 均服从高斯分布, 于是我们可以计算解析解:先考虑一维情况: $$\\begin{aligned}D_{KL}(\\mathcal{N}\\left(\\mu,\\sigma^{2}\\right)|\\mathcal{N}(0,1))&amp;=\\int_z\\frac1{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(z-\\mu\\right)^2}{2\\sigma^2}\\right)\\log\\frac1{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(z-\\mu\\right)^2}{2\\sigma^2}\\right)\\&amp;=\\int_z\\left(\\frac{-\\left(z-\\mu\\right)^2}{2\\sigma^2}+\\frac{z^2}2-\\log\\sigma\\right)\\mathcal{N}\\left(\\mu,\\sigma^2\\right)dz\\&amp;=-\\int_z\\frac{\\left(z-\\mu\\right)^2}{2\\sigma^2}\\mathcal{N}\\left(\\mu,\\sigma^2\\right)dz+\\int_z\\frac{z^2}2\\mathcal{N}\\left(\\mu,\\sigma^2\\right)dz-\\int_z\\log\\sigma\\mathcal{N}\\left(\\mu,\\sigma^2\\right)dz\\&amp;=-\\frac{\\mathbb{E}\\left[\\left(z-\\mu\\right)^2\\right]}{2\\sigma^2}+\\frac{\\mathbb{E}\\left[z^2\\right]}2-\\log\\sigma\\&amp;=\\frac12(-1+\\sigma^2+\\mu^2-\\log\\sigma^2)\\end{aligned}$$ 其中 $\\mathbb{E}[]$ 表示信息熵 推广到 $d$ 维: $$D_{KL}:(q_\\phi(z\\mid X),p(z))=\\sum_{j=1}^d\\frac{1}{2}(-1+\\sigma^{(j)^2}+\\mu^{(j)^2}-\\log\\sigma^{(j)^2})$$ 其中 ${a^{(j)}}^2$ 代表向量 $a$ 的第 $j$ 个元素的平方 现在只剩最后一个问题了: 怎么求右边那项 $\\mathbb{E}{q{\\phi}}:[\\log p_{\\theta}(X\\mid z)]$这一项称为重构损失 ($Reconstruction\\ Loss$), 我们从 $q_\\phi(z\\mid X)$ 中采样多个 $z_i$ 来近似求解这一项即: $$\\mathbb{E}{q_\\phi}\\left[\\log p_\\theta(X\\mid z)\\right]\\approx\\frac{1}{m}\\sum{i=1}^m\\log p_\\theta\\left(X\\mid z_i\\right)$$ 其中 $z_i\\sim q_\\phi\\left(z\\mid x_i\\right)=\\mathcal{N}\\left(z\\mid\\mu\\left(x_i;\\phi\\right),\\sigma^2\\left(x_i;\\phi\\right)*I\\right)$ 然后我们假设数据 $x_i$ 是 $K$ 维的, 就能用 $MSE$ , 得到: $$\\begin{aligned}\\log p_{\\theta}\\left(X\\mid z_{i}\\right)&amp;=\\log\\frac{\\exp\\left(-\\frac12(X-\\mu^{\\prime})^\\mathrm{T}\\Sigma^{\\prime-1}(X-\\mu^{\\prime})\\right)}{\\sqrt{(2\\pi)^k|\\Sigma^{\\prime}|}}\\&amp;=-\\frac12(X-\\mu^{\\prime})^\\mathrm{T}\\Sigma^{\\prime-1}(X-\\mu^{\\prime})-\\log\\sqrt{(2\\pi)^k|\\Sigma^{\\prime}|}\\&amp;=-\\frac12\\sum_{k=1}^K\\frac{(X^{(k)}-\\mu^{\\prime{(k)}})^2}{\\sigma^{\\prime{(k)}}}-\\log\\sqrt{(2\\pi)^K\\prod_{k=1}^K\\sigma^{\\prime{(k)}}}\\end{aligned}$$ 这样我们单独损失函数的每一块都算好了 为了损失函数反应样本量, 取一个平均,于是损失函数就表示为: $$\\begin{aligned}\\mathcal{L}&amp;=-\\frac1n\\sum_{i=1}^n\\ell(p_\\theta,q_\\phi)\\&amp;=\\frac1n\\sum_{i=1}^nD_{KL}\\left(q_\\phi,p\\right)-\\frac1n\\sum_{i=1}^n\\mathbb{E}{q_\\phi}\\left[\\log p_\\theta(x_i\\mid z)\\right]\\&amp;=\\frac1n\\sum{i=1}^nD_{KL}\\left(q_\\phi,p\\right)-\\frac1{nm}\\sum_{i=1}^n\\sum_{j=1}^m\\log p_\\theta\\left(x_i\\mid z_j\\right)\\end{aligned}$$ 我们通过从 $q_\\phi(z\\mid x_i)$ 中采样 $m$ 次 $z_j$, 来逼近 $\\mathbb{E}_{q\\phi}\\left[\\log p_\\theta(x_i\\mid z)\\right]$ 你可能会问: 为什么计算 $Encoder$ 的时候我们不用采样的方法, 而是使用了贝叶斯变分推断去优化 $KL$ 散度, 而计算 $Decoder$ 的时候却使用采样的方法 这是因为我们其实是从 $q_\\phi(z\\mid x_i)$ 中采样得到 $z_j$, 随着网络的训练，近似后验 $q_\\phi(z\\mid x_i)$,很快就会比较接近真实的后验分布, 这样一来，我们有很大可能能够在有限次数的采样中，采样到与 $x_i$ 关联的 $z_{j}$, 所以可以这样采样 而在 $Encoder$ 那里, 我们还没有计算出 $z_{i}$, 就无从知道哪个 $x_{i}$ 和哪个 $z_{i}$ 相关, 所以没有办法使用采样的方法 (因为空间是很大的, 采样无法模拟真实情况) 我们把前面算的似然度代进去展开得到: $$\\begin{aligned}\\mathcal{L}&amp;=\\frac 1 n\\sum*{i=1}^nD*{KL}\\left (q\\phi, p\\right)-\\frac 1 n\\sum{i=1}^n\\log p\\theta\\left (x_i\\mid z_i\\right)\\&amp;=\\frac 1 n\\sum{i=1}^n\\sum*{j=1}^d\\frac 12 (-1+\\sigma_i^{(j)^2}+\\mu_i^{(j)^2}-\\log\\sigma_i^{(j)^2})\\&amp;-\\frac 1 n\\sum*{i=1}^n\\left (-\\frac 12\\sum*{k=1}^K\\frac{(x_i^{(k)}-\\mu_i^{\\prime (k)})^2}{\\sigma_i^{\\prime (k)}}-\\log\\sqrt{(2\\pi)^K\\prod*{k=1}^K\\sigma*i^{\\prime (k)}}\\right)\\end{aligned}$$ 我们是不需要用到超参数 $\\sigma’$ 的, 不妨假设为 $\\frac{1}{2}$, 于是就得到了论文里很漂亮的损失函数: $$\\mathcal{L}=\\frac1n\\sum*{i=1}^n\\sum*{j=1}^d\\frac12(-1+\\sigma_i^{(j)^2}+\\mu_i^{(j)^2}-\\log\\sigma_i^{(j)^2})+\\frac1n\\sum*{i=1}^n|x_i-\\mu_i^{\\prime}|^2$$ 其中，$x_i$ 代表第 $i$ 个样本，是 Encoder 的输入$\\mu_i$ 和 $\\sigma_i^2$ 是 Encoder 的输出，代表 $z\\mid x_i$ 的分布的参数$z_i$ 是从 $z\\mid x_i$ 中采样得到的一个样本，它是 Decoder 的输入$\\mu_i^{\\prime}$ 是 Decoder 的输出，代表利用 $z_i$ 解码后对应的数据点 $\\tilde{x}_i$ 现在我们可以对这个模型有新的观点了:![[Pasted image 20240511124346.png]]我们输入样本, $Encoder$ 拟合出后验分布, 然后我们施加噪声采样 (重参数技巧), 将采样结果送入 $Decoder$ 拟合出真实分布, 不过我们一般只要均值而不要方差, 直接输出均值 右边一项, 就是希望最小化 $Decoder$ 产出和 $Encoder$ 输入的差距, 这和我们的自编码器的思想是一样的 到这里，我们终于得到了在假设先验、后验、似然均是高斯分布的情况下，$VAE$ 最终的损失函数, 可喜可贺, 可喜可贺 最后, 我们只要用随机梯度下降去优化这个问题就行了 (其他也可以) 代码代码我写在 $colab$ 上了, 方便大家如果没有电脑也能学习, 点击链接访问: 香草味自编码器:自编码器讲解代码 - Colab (google.com) 卷积变分自编码器cvae.ipynb - Colab (google.com) 参考文献: 原论文 系列完","link":"/2024/04/28/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"}],"tags":[],"categories":[],"pages":[{"title":"关于我","text":"PROF1LE这里是空的!","link":"/about/index.html"},{"title":"友情链接","text":"这里是空的!","link":"/links/index.html"}]}